{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements BitFlip env from Andrychowicz et al. '17\n",
    "(https://arxiv.org/abs/1707.01495)\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class BitFlip(gym.GoalEnv):\n",
    "    \"\"\"\n",
    "    Simple BitFlipping environment for goal-conditioned reinforcement\n",
    "    learning.\n",
    "\n",
    "    BitFlipEnv is a Goal-conditioned environment where the agent must reach\n",
    "    a goal state of flipped bits from the current state, only receiving a\n",
    "    sparse reward of -1 or 0 depending on if the goal state is reached.\n",
    "\n",
    "    The environment follows the toy motivating example presented in\n",
    "    Andrychowicz et al. '17, where at each step, the agent can choose\n",
    "    whether which bit in the string it will flip the value for. The episode\n",
    "    terminates when either the goal state has been reached, or num_bits steps\n",
    "    have been taken.\n",
    "    \n",
    "    An alternate version of this environment is a 2D action space where the \n",
    "    agent can only flip the ith bit, where i is the step count in the episode.\n",
    "\n",
    "    See Section 3.1 from Andrychowicz et al. for the full description.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_bits=10, select_bit=True, succ_bonus=11):\n",
    "        \"\"\"\n",
    "        Initializes Bit Flipping environment instance\n",
    "        Args:\n",
    "          num_bits: length of 1D binary string for state and goal\n",
    "          flip_mode: whether or not agent can choose which bit to flip\n",
    "        \"\"\"\n",
    "        self.num_bits = num_bits\n",
    "        self.select_bit = select_bit\n",
    "        self.succ_bonus = succ_bonus\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"observation\": spaces.MultiBinary(num_bits),\n",
    "            \"desired_goal\": spaces.MultiBinary(num_bits),\n",
    "            \"achieved_goal\": spaces.MultiBinary(num_bits)})\n",
    "        if select_bit:\n",
    "            self.action_space = spaces.Discrete(num_bits)\n",
    "        else:\n",
    "            self.action_space = spaces.Discrete(2)\n",
    "        self.state, self.goal, self.current_step = None, None, None\n",
    "        self.done = False\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.done = False\n",
    "        self.current_step = 0\n",
    "        self.state = self.np_random.randint(2, size=self.num_bits)\n",
    "        self.goal = self.np_random.randint(2, size=self.num_bits)\n",
    "        return {'observation': self.state,\n",
    "                'desired_goal': self.goal,\n",
    "                'achieved_goal': self.state}\n",
    "\n",
    "    def _is_success(self, achieved_goal, desired_goal):\n",
    "        return np.all(achieved_goal == desired_goal)\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in self.action_space, 'action {} invalid'.format(action)\n",
    "        if self.done:\n",
    "            print('Episode has ended, need to call reset()')\n",
    "        else:\n",
    "            select_idx = None\n",
    "            if self.select_bit:\n",
    "                select_idx = action\n",
    "            elif action:\n",
    "                select_idx = self.current_step\n",
    "            if select_idx is not None:\n",
    "                self.state[select_idx] = int(not self.state[select_idx])\n",
    "            self.current_step += 1\n",
    "        obs = {'observation': self.state.copy(),\n",
    "               'desired_goal': self.goal.copy(),\n",
    "               'achieved_goal': self.state.copy()}\n",
    "        info = {\n",
    "            'is_success': self._is_success(obs['achieved_goal'], self.goal),\n",
    "            'current_step': self.current_step-1\n",
    "        }\n",
    "        reward = self.compute_reward(self.state, self.goal, info)\n",
    "        self.done = done = (self.current_step == self.num_bits) or (reward == 0)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
    "        succ = self._is_success(achieved_goal, desired_goal)\n",
    "        if not self.select_bit:\n",
    "            succ += self.succ_bonus\n",
    "        return succ - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 67       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 990      |\n",
      "| success rate            | 0.0101   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 34       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 1990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 2990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 3990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 4990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 5990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 6990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 7990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 8990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 9990     |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 10990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 11990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 12990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 13990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1500     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 14990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1600     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 15990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1700     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 16990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1800     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 17990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1900     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 18990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2000     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 19990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2100     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 20990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2200     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 21990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2300     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 22990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2400     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 23990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2500     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 24990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2600     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 25990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2700     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 26990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2800     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 27990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 2900     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 28990    |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 3000     |\n",
      "| mean 100 episode reward | 100      |\n",
      "| steps                   | 29990    |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.deepq.dqn.DQN at 0x7f59eb9aa8d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines import HER, DQN\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "\n",
    "exp_root = './experiments'\n",
    "hms_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "exp_name = 'HER_BitFlip'\n",
    "\n",
    "exp_dir = osp.join(exp_root, exp_name, hms_time)\n",
    "os.makedirs(exp_dir)\n",
    "\n",
    "env = BitFlip(10, False)\n",
    "\n",
    "model = HER('MlpPolicy', env, DQN, n_sampled_goal=4,\n",
    "            tensorboard_log=exp_dir,\n",
    "            goal_selection_strategy='episode',\n",
    "            verbose=1)\n",
    "\n",
    "model.learn(int(3e4),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -10      |\n",
      "| steps                   | 990      |\n",
      "| success rate            | 0        |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import HER, DQN, SAC, DDPG, TD3\n",
    "from stable_baselines.her import GoalSelectionStrategy, HERGoalEnvWrapper\n",
    "from stable_baselines.common.bit_flipping_env import BitFlippingEnv\n",
    "\n",
    "N_BITS = 10\n",
    "\n",
    "model_class = DQN  # works also with SAC, DDPG and TD3\n",
    "\n",
    "env = BitFlippingEnv(N_BITS, continuous=model_class in [DDPG, SAC, TD3], max_steps=N_BITS)\n",
    "\n",
    "# Available strategies (cf paper): future, final, episode, random\n",
    "goal_selection_strategy = 'future' # equivalent to GoalSelectionStrategy.FUTURE\n",
    "\n",
    "# Wrap the model\n",
    "model = HER('MlpPolicy', env, model_class, n_sampled_goal=4, goal_selection_strategy=goal_selection_strategy,\n",
    "                                                verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save(\"./experiments/her_bit_env\")\n",
    "\n",
    "# WARNING: you must pass an env\n",
    "# or wrap your environment with HERGoalEnvWrapper to use the predict method\n",
    "model = HER.load('./experiments/her_bit_env', env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| % time spent exploring  | 90       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -10      |\n",
      "| steps                   | 988      |\n",
      "| success rate            | 0.0101   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 80       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | -9.9     |\n",
      "| steps                   | 1985     |\n",
      "| success rate            | 0.02     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 70       |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -9.9     |\n",
      "| steps                   | 2976     |\n",
      "| success rate            | 0.01     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 61       |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -9.7     |\n",
      "| steps                   | 3953     |\n",
      "| success rate            | 0.07     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 51       |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -9.4     |\n",
      "| steps                   | 4901     |\n",
      "| success rate            | 0.11     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 43       |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -8.1     |\n",
      "| steps                   | 5752     |\n",
      "| success rate            | 0.45     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 35       |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -7.3     |\n",
      "| steps                   | 6536     |\n",
      "| success rate            | 0.57     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 28       |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -7       |\n",
      "| steps                   | 7305     |\n",
      "| success rate            | 0.65     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 21       |\n",
      "| episodes                | 900      |\n",
      "| mean 100 episode reward | -6.5     |\n",
      "| steps                   | 8035     |\n",
      "| success rate            | 0.8      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 14       |\n",
      "| episodes                | 1000     |\n",
      "| mean 100 episode reward | -5.7     |\n",
      "| steps                   | 8693     |\n",
      "| success rate            | 0.85     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 9        |\n",
      "| episodes                | 1100     |\n",
      "| mean 100 episode reward | -4.8     |\n",
      "| steps                   | 9275     |\n",
      "| success rate            | 0.97     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 3        |\n",
      "| episodes                | 1200     |\n",
      "| mean 100 episode reward | -4.4     |\n",
      "| steps                   | 9817     |\n",
      "| success rate            | 0.98     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1300     |\n",
      "| mean 100 episode reward | -4.3     |\n",
      "| steps                   | 10343    |\n",
      "| success rate            | 1        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 1400     |\n",
      "| mean 100 episode reward | -4       |\n",
      "| steps                   | 10846    |\n",
      "| success rate            | 1        |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0a9a79bb8905>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/her/her.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    111\u001b[0m         return self.model.learn(total_timesteps, callback=callback, log_interval=log_interval,\n\u001b[1;32m    112\u001b[0m                                 \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                                 replay_wrapper=self.replay_wrapper)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                         _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n\u001b[0;32m--> 295\u001b[0;31m                                                         sess=self.sess)\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprioritized_replay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/common/tf_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ = 0\n",
    "obs = env.reset()\n",
    "n_eps = 0\n",
    "while n_eps < 20:\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, i = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        n_eps += 1\n",
    "        succ += reward == 0\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "succ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

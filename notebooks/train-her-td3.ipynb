{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines import HER, TD3\n",
    "from stable_baselines.common.atari_wrappers import FrameStack\n",
    "from rrc_simulation.gym_wrapper.envs import custom_env\n",
    "from spinup.utils import rrc_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reorient_env():\n",
    "    info_keys = ['is_success', 'is_success_ori_dist', 'dist', 'final_dist', 'final_score',\n",
    "                 'final_ori_dist']\n",
    "\n",
    "    wrappers = [gym.wrappers.ClipAction,\n",
    "                {'cls': custom_env.LogInfoWrapper,\n",
    "                 'kwargs': dict(info_keys=info_keys)},\n",
    "                {'cls': custom_env.CubeRewardWrapper,\n",
    "                 'kwargs': dict(pos_coef=1., ori_coef=1.,\n",
    "                                ac_norm_pen=0.2, rew_fn='exp',\n",
    "                                goal_env=True)},\n",
    "                {'cls': custom_env.ReorientWrapper,\n",
    "                 'kwargs': dict(goal_env=True)},\n",
    "                {'cls': gym.wrappers.TimeLimit,\n",
    "                 'kwargs': dict(max_episode_steps=rrc_utils.EPLEN)},\n",
    "                custom_env.FlattenGoalWrapper]\n",
    "    initializer = custom_env.ReorientInitializer(1, 0.1)\n",
    "    env_fn = rrc_utils.make_env_fn('real_robot_challenge_phase_1-v1', wrapper_params=wrappers,\n",
    "                                   action_type=rrc_utils.action_type,\n",
    "                                   initializer=initializer,\n",
    "                                   frameskip=rrc_utils.FRAMESKIP,\n",
    "                                   visualization=False)\n",
    "    env = env_fn()\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_curr_env():\n",
    "    info_keys = ['is_success_ori_dist', 'dist', 'final_dist', 'final_score',\n",
    "                 'final_ori_dist', 'goal_sample_radius',\n",
    "                 'init_sample_radius']\n",
    "\n",
    "    wrappers = [gym.wrappers.ClipAction, \n",
    "                {'cls': custom_env.LogInfoWrapper,\n",
    "                 'kwargs': dict(info_keys=info_keys)},\n",
    "                {'cls': gym.wrappers.TimeLimit,\n",
    "                 'kwargs': dict(max_episode_steps=rrc_utils.EPLEN)},\n",
    "                custom_env.FlattenGoalWrapper,]\n",
    "\n",
    "    env_fn = rrc_utils.make_env_fn('real_robot_challenge_phase_1-v4', wrapper_params=wrappers,\n",
    "                                   action_type=rrc_utils.action_type,\n",
    "                                   initializer=rrc_utils.push_curr_initializer,\n",
    "                                   frameskip=rrc_utils.FRAMESKIP,\n",
    "                                   visualization=False)\n",
    "    env = env_fn()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_reorient_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.her.her.HER at 0x7f291421d9e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_dir = './data/HER-SAC_sparse_push/2020-09-18_12-28-22/'\n",
    "load_dir = osp.join(exp_dir, '2e6-steps.zip') # './data/HER-SAC_push_reorient/2020-09-20_15-58-02/1e6-steps.zip'\n",
    "model = HER('MlpPolicy', env, TD3, n_sampled_goal=4,\n",
    "            tensorboard_log=exp_dir,\n",
    "            goal_selection_strategy='future',\n",
    "            verbose=1, buffer_size=int(1e6),\n",
    "            learning_rate=3e-4,\n",
    "            gamma=0.95, batch_size=100,\n",
    "            policy_kwargs=dict(layers=[256, 256]))\n",
    "model.load(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n_eps = 10\n",
    "final_infos = []\n",
    "\n",
    "for _ in range(n_eps):\n",
    "    d = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    r_total = 0\n",
    "    while not d:\n",
    "        obs, r, d, i = env.step(model.predict(obs)[0])\n",
    "        r_total += r\n",
    "    i['total_rew'] = r_total\n",
    "    final_infos.append(i)\n",
    "    \n",
    "print('total_rew:', np.mean([i['total_rew'] for i in final_infos]),\n",
    "      'final_dist min:', np.min([i['final_dist'] for i in final_infos]), \n",
    "      'final_dist mean:', np.mean([i['final_dist'] for i in final_infos]), \n",
    "      'final_dist std:', np.std([i['final_dist'] for i in final_infos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| current_lr              | 0.0003    |\n",
      "| episodes                | 100       |\n",
      "| fps                     | 35        |\n",
      "| mean 100 episode reward | 477       |\n",
      "| n_updates               | 37200     |\n",
      "| qf1_loss                | 1.0435271 |\n",
      "| qf2_loss                | 1.0081202 |\n",
      "| success rate            | 0         |\n",
      "| time_elapsed            | 1068      |\n",
      "| total timesteps         | 37500     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| current_lr              | 0.0003    |\n",
      "| episodes                | 200       |\n",
      "| fps                     | 37        |\n",
      "| mean 100 episode reward | 385       |\n",
      "| n_updates               | 74700     |\n",
      "| qf1_loss                | 1.0728462 |\n",
      "| qf2_loss                | 1.0651686 |\n",
      "| success rate            | 0         |\n",
      "| time_elapsed            | 1989      |\n",
      "| total timesteps         | 75000     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 300        |\n",
      "| fps                     | 38         |\n",
      "| mean 100 episode reward | 409        |\n",
      "| n_updates               | 112200     |\n",
      "| qf1_loss                | 0.7260096  |\n",
      "| qf2_loss                | 0.74185723 |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 2953       |\n",
      "| total timesteps         | 112500     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| current_lr              | 0.0003    |\n",
      "| episodes                | 400       |\n",
      "| fps                     | 37        |\n",
      "| mean 100 episode reward | 453       |\n",
      "| n_updates               | 149700    |\n",
      "| qf1_loss                | 0.6087902 |\n",
      "| qf2_loss                | 0.6203797 |\n",
      "| success rate            | 0         |\n",
      "| time_elapsed            | 3994      |\n",
      "| total timesteps         | 150000    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 500        |\n",
      "| fps                     | 35         |\n",
      "| mean 100 episode reward | 413        |\n",
      "| n_updates               | 187200     |\n",
      "| qf1_loss                | 0.3719711  |\n",
      "| qf2_loss                | 0.37653813 |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 5216       |\n",
      "| total timesteps         | 187500     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 600        |\n",
      "| fps                     | 33         |\n",
      "| mean 100 episode reward | 476        |\n",
      "| n_updates               | 224700     |\n",
      "| qf1_loss                | 0.30058962 |\n",
      "| qf2_loss                | 0.2836238  |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 6632       |\n",
      "| total timesteps         | 225000     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 800        |\n",
      "| fps                     | 32         |\n",
      "| mean 100 episode reward | 438        |\n",
      "| n_updates               | 299700     |\n",
      "| qf1_loss                | 0.36734456 |\n",
      "| qf2_loss                | 0.36225086 |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 9370       |\n",
      "| total timesteps         | 300000     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 900        |\n",
      "| fps                     | 31         |\n",
      "| mean 100 episode reward | 504        |\n",
      "| n_updates               | 337200     |\n",
      "| qf1_loss                | 0.98321265 |\n",
      "| qf2_loss                | 0.99125725 |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 10786      |\n",
      "| total timesteps         | 337500     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 1000       |\n",
      "| fps                     | 30         |\n",
      "| mean 100 episode reward | 433        |\n",
      "| n_updates               | 374700     |\n",
      "| qf1_loss                | 0.56616026 |\n",
      "| qf2_loss                | 0.5728342  |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 12257      |\n",
      "| total timesteps         | 375000     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 1100       |\n",
      "| fps                     | 30         |\n",
      "| mean 100 episode reward | 516        |\n",
      "| n_updates               | 412200     |\n",
      "| qf1_loss                | 0.6225643  |\n",
      "| qf2_loss                | 0.61136496 |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 13747      |\n",
      "| total timesteps         | 412500     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0003     |\n",
      "| episodes                | 1200       |\n",
      "| fps                     | 29         |\n",
      "| mean 100 episode reward | 498        |\n",
      "| n_updates               | 449700     |\n",
      "| qf1_loss                | 0.48386315 |\n",
      "| qf2_loss                | 0.48407096 |\n",
      "| success rate            | 0          |\n",
      "| time_elapsed            | 15253      |\n",
      "| total timesteps         | 450000     |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "exp_root = './data'\n",
    "hms_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "exp_name = 'HER-TD3_push_reorient'\n",
    "exp_dir = osp.join(exp_root, exp_name, hms_time)\n",
    "os.makedirs(exp_dir)\n",
    "\n",
    "model = HER('MlpPolicy', env, TD3, n_sampled_goal=4,\n",
    "            tensorboard_log=exp_dir,\n",
    "            goal_selection_strategy='future',\n",
    "            verbose=1, buffer_size=int(1e6),\n",
    "            learning_rate=3e-4,\n",
    "            gamma=0.95, batch_size=100,\n",
    "            policy_kwargs=dict(layers=[256, 256]))\n",
    "\n",
    "# Train for 1e6 steps\n",
    "model.learn(int(1e6),)\n",
    "# Save the trained agent\n",
    "model.save(osp.join(exp_dir, '1e6-steps'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

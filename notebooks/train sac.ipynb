{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import HER, SAC\n",
    "from stable_baselines.common.atari_wrappers import FrameStack\n",
    "import three_finger.envs\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('pddm_dclaw_turn-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.88620865 |\n",
      "| ent_coef_loss           | -1.6172013 |\n",
      "| entropy                 | 9.947889   |\n",
      "| episodes                | 4          |\n",
      "| fps                     | 63         |\n",
      "| mean 100 episode reward | -5.83e+03  |\n",
      "| n_updates               | 1245       |\n",
      "| policy_loss             | 23.8154    |\n",
      "| qf1_loss                | 4.2142134  |\n",
      "| qf2_loss                | 3.9545045  |\n",
      "| time_elapsed            | 23         |\n",
      "| total timesteps         | 1500       |\n",
      "| value_loss              | 6.3569393  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.7399539  |\n",
      "| ent_coef_loss           | -2.9220428 |\n",
      "| entropy                 | 7.961372   |\n",
      "| episodes                | 8          |\n",
      "| fps                     | 58         |\n",
      "| mean 100 episode reward | -7.25e+03  |\n",
      "| n_updates               | 3245       |\n",
      "| policy_loss             | 105.16783  |\n",
      "| qf1_loss                | 15.852928  |\n",
      "| qf2_loss                | 16.211555  |\n",
      "| time_elapsed            | 60         |\n",
      "| total timesteps         | 3500       |\n",
      "| value_loss              | 16.078577  |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| current_lr              | 0.0001    |\n",
      "| ent_coef                | 0.624183  |\n",
      "| ent_coef_loss           | -3.384873 |\n",
      "| entropy                 | 6.2103386 |\n",
      "| episodes                | 12        |\n",
      "| fps                     | 57        |\n",
      "| mean 100 episode reward | -7.36e+03 |\n",
      "| n_updates               | 5245      |\n",
      "| policy_loss             | 180.6636  |\n",
      "| qf1_loss                | 73.39847  |\n",
      "| qf2_loss                | 56.64701  |\n",
      "| time_elapsed            | 96        |\n",
      "| total timesteps         | 5500      |\n",
      "| value_loss              | 37.96739  |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5289948  |\n",
      "| ent_coef_loss           | -2.8884544 |\n",
      "| entropy                 | 4.6524673  |\n",
      "| episodes                | 16         |\n",
      "| fps                     | 56         |\n",
      "| mean 100 episode reward | -7.68e+03  |\n",
      "| n_updates               | 7245       |\n",
      "| policy_loss             | 268.61594  |\n",
      "| qf1_loss                | 90.66548   |\n",
      "| qf2_loss                | 80.53614   |\n",
      "| time_elapsed            | 133        |\n",
      "| total timesteps         | 7500       |\n",
      "| value_loss              | 46.756588  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.45918334 |\n",
      "| ent_coef_loss           | -2.3442984 |\n",
      "| entropy                 | 3.4364736  |\n",
      "| episodes                | 20         |\n",
      "| fps                     | 55         |\n",
      "| mean 100 episode reward | -8.04e+03  |\n",
      "| n_updates               | 9245       |\n",
      "| policy_loss             | 407.59058  |\n",
      "| qf1_loss                | 96.3554    |\n",
      "| qf2_loss                | 106.13855  |\n",
      "| time_elapsed            | 172        |\n",
      "| total timesteps         | 9500       |\n",
      "| value_loss              | 65.97338   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.41133752  |\n",
      "| ent_coef_loss           | -0.56977046 |\n",
      "| entropy                 | 1.8862975   |\n",
      "| episodes                | 24          |\n",
      "| fps                     | 55          |\n",
      "| mean 100 episode reward | -7.65e+03   |\n",
      "| n_updates               | 11245       |\n",
      "| policy_loss             | 409.55734   |\n",
      "| qf1_loss                | 112.11736   |\n",
      "| qf2_loss                | 117.240395  |\n",
      "| time_elapsed            | 208         |\n",
      "| total timesteps         | 11500       |\n",
      "| value_loss              | 90.61428    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.3784935    |\n",
      "| ent_coef_loss           | 0.0151168965 |\n",
      "| entropy                 | 1.2512126    |\n",
      "| episodes                | 28           |\n",
      "| fps                     | 55           |\n",
      "| mean 100 episode reward | -7.35e+03    |\n",
      "| n_updates               | 13245        |\n",
      "| policy_loss             | 416.72772    |\n",
      "| qf1_loss                | 157.44815    |\n",
      "| qf2_loss                | 180.12743    |\n",
      "| time_elapsed            | 245          |\n",
      "| total timesteps         | 13500        |\n",
      "| value_loss              | 132.5355     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.35335758  |\n",
      "| ent_coef_loss           | -0.29411903 |\n",
      "| entropy                 | 0.826514    |\n",
      "| episodes                | 32          |\n",
      "| fps                     | 55          |\n",
      "| mean 100 episode reward | -7.17e+03   |\n",
      "| n_updates               | 15245       |\n",
      "| policy_loss             | 445.68436   |\n",
      "| qf1_loss                | 237.31635   |\n",
      "| qf2_loss                | 240.34622   |\n",
      "| time_elapsed            | 280         |\n",
      "| total timesteps         | 15500       |\n",
      "| value_loss              | 185.83871   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.36409685 |\n",
      "| ent_coef_loss           | 0.37193942 |\n",
      "| entropy                 | 0.97794354 |\n",
      "| episodes                | 36         |\n",
      "| fps                     | 55         |\n",
      "| mean 100 episode reward | -7.34e+03  |\n",
      "| n_updates               | 17245      |\n",
      "| policy_loss             | 471.34772  |\n",
      "| qf1_loss                | 454.51828  |\n",
      "| qf2_loss                | 488.6902   |\n",
      "| time_elapsed            | 317        |\n",
      "| total timesteps         | 17500      |\n",
      "| value_loss              | 114.404526 |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.34665006   |\n",
      "| ent_coef_loss           | -0.097117014 |\n",
      "| entropy                 | 0.5811794    |\n",
      "| episodes                | 40           |\n",
      "| fps                     | 55           |\n",
      "| mean 100 episode reward | -7.01e+03    |\n",
      "| n_updates               | 19245        |\n",
      "| policy_loss             | 526.12024    |\n",
      "| qf1_loss                | 287.63193    |\n",
      "| qf2_loss                | 294.17114    |\n",
      "| time_elapsed            | 353          |\n",
      "| total timesteps         | 19500        |\n",
      "| value_loss              | 119.10598    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.32507265  |\n",
      "| ent_coef_loss           | -0.13629746 |\n",
      "| entropy                 | -0.66125363 |\n",
      "| episodes                | 44          |\n",
      "| fps                     | 55          |\n",
      "| mean 100 episode reward | -7.49e+03   |\n",
      "| n_updates               | 21245       |\n",
      "| policy_loss             | 602.97144   |\n",
      "| qf1_loss                | 2704.082    |\n",
      "| qf2_loss                | 2677.323    |\n",
      "| time_elapsed            | 388         |\n",
      "| total timesteps         | 21500       |\n",
      "| value_loss              | 114.779655  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.31849307  |\n",
      "| ent_coef_loss           | -0.18848649 |\n",
      "| entropy                 | -0.92909867 |\n",
      "| episodes                | 48          |\n",
      "| fps                     | 55          |\n",
      "| mean 100 episode reward | -7.44e+03   |\n",
      "| n_updates               | 23245       |\n",
      "| policy_loss             | 606.62415   |\n",
      "| qf1_loss                | 2909.6123   |\n",
      "| qf2_loss                | 2923.4917   |\n",
      "| time_elapsed            | 425         |\n",
      "| total timesteps         | 23500       |\n",
      "| value_loss              | 145.48672   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.3366795   |\n",
      "| ent_coef_loss           | 0.052292667 |\n",
      "| entropy                 | -0.90250427 |\n",
      "| episodes                | 52          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -7.34e+03   |\n",
      "| n_updates               | 25245       |\n",
      "| policy_loss             | 624.79785   |\n",
      "| qf1_loss                | 1820.2897   |\n",
      "| qf2_loss                | 1645.3809   |\n",
      "| time_elapsed            | 463         |\n",
      "| total timesteps         | 25500       |\n",
      "| value_loss              | 133.90175   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.3465848   |\n",
      "| ent_coef_loss           | -0.27255464 |\n",
      "| entropy                 | -0.77614003 |\n",
      "| episodes                | 56          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -7.26e+03   |\n",
      "| n_updates               | 27245       |\n",
      "| policy_loss             | 675.4419    |\n",
      "| qf1_loss                | 195.75204   |\n",
      "| qf2_loss                | 232.76846   |\n",
      "| time_elapsed            | 502         |\n",
      "| total timesteps         | 27500       |\n",
      "| value_loss              | 133.21123   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.3533301   |\n",
      "| ent_coef_loss           | -0.20802584 |\n",
      "| entropy                 | -1.0392971  |\n",
      "| episodes                | 60          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -7.07e+03   |\n",
      "| n_updates               | 29245       |\n",
      "| policy_loss             | 724.69946   |\n",
      "| qf1_loss                | 244.507     |\n",
      "| qf2_loss                | 262.5503    |\n",
      "| time_elapsed            | 540         |\n",
      "| total timesteps         | 29500       |\n",
      "| value_loss              | 118.920685  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.34395885  |\n",
      "| ent_coef_loss           | -0.11286394 |\n",
      "| entropy                 | -1.4295399  |\n",
      "| episodes                | 64          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.83e+03   |\n",
      "| n_updates               | 31245       |\n",
      "| policy_loss             | 695.9584    |\n",
      "| qf1_loss                | 298.16617   |\n",
      "| qf2_loss                | 300.69516   |\n",
      "| time_elapsed            | 578         |\n",
      "| total timesteps         | 31500       |\n",
      "| value_loss              | 129.169     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.33927485  |\n",
      "| ent_coef_loss           | -0.33943725 |\n",
      "| entropy                 | -1.4366684  |\n",
      "| episodes                | 68          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.88e+03   |\n",
      "| n_updates               | 33245       |\n",
      "| policy_loss             | 717.44885   |\n",
      "| qf1_loss                | 274.44537   |\n",
      "| qf2_loss                | 261.75122   |\n",
      "| time_elapsed            | 613         |\n",
      "| total timesteps         | 33500       |\n",
      "| value_loss              | 228.38264   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.35029382 |\n",
      "| ent_coef_loss           | 0.47096446 |\n",
      "| entropy                 | -1.6126782 |\n",
      "| episodes                | 72         |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -6.66e+03  |\n",
      "| n_updates               | 35245      |\n",
      "| policy_loss             | 775.19305  |\n",
      "| qf1_loss                | 1730.886   |\n",
      "| qf2_loss                | 1815.0555  |\n",
      "| time_elapsed            | 650        |\n",
      "| total timesteps         | 35500      |\n",
      "| value_loss              | 168.44034  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.3640957  |\n",
      "| ent_coef_loss           | 0.24783882 |\n",
      "| entropy                 | -1.2821985 |\n",
      "| episodes                | 76         |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -6.67e+03  |\n",
      "| n_updates               | 37245      |\n",
      "| policy_loss             | 771.9916   |\n",
      "| qf1_loss                | 375.46     |\n",
      "| qf2_loss                | 367.50778  |\n",
      "| time_elapsed            | 686        |\n",
      "| total timesteps         | 37500      |\n",
      "| value_loss              | 177.07452  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.38377568 |\n",
      "| ent_coef_loss           | 0.66035557 |\n",
      "| entropy                 | -1.0501693 |\n",
      "| episodes                | 80         |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -6.83e+03  |\n",
      "| n_updates               | 39245      |\n",
      "| policy_loss             | 828.7666   |\n",
      "| qf1_loss                | 2442.3167  |\n",
      "| qf2_loss                | 2438.4985  |\n",
      "| time_elapsed            | 724        |\n",
      "| total timesteps         | 39500      |\n",
      "| value_loss              | 185.13356  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.39097422  |\n",
      "| ent_coef_loss           | -0.14285775 |\n",
      "| entropy                 | -1.1396043  |\n",
      "| episodes                | 84          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.78e+03   |\n",
      "| n_updates               | 41245       |\n",
      "| policy_loss             | 850.844     |\n",
      "| qf1_loss                | 453.18594   |\n",
      "| qf2_loss                | 382.10754   |\n",
      "| time_elapsed            | 760         |\n",
      "| total timesteps         | 41500       |\n",
      "| value_loss              | 176.25931   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.40367976 |\n",
      "| ent_coef_loss           | 0.42469913 |\n",
      "| entropy                 | -0.7982409 |\n",
      "| episodes                | 88         |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -6.77e+03  |\n",
      "| n_updates               | 43245      |\n",
      "| policy_loss             | 849.92804  |\n",
      "| qf1_loss                | 471.97736  |\n",
      "| qf2_loss                | 431.88638  |\n",
      "| time_elapsed            | 800        |\n",
      "| total timesteps         | 43500      |\n",
      "| value_loss              | 233.42267  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.4139311   |\n",
      "| ent_coef_loss           | -0.2916807  |\n",
      "| entropy                 | -0.66165996 |\n",
      "| episodes                | 92          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.77e+03   |\n",
      "| n_updates               | 45245       |\n",
      "| policy_loss             | 885.9666    |\n",
      "| qf1_loss                | 412.8753    |\n",
      "| qf2_loss                | 408.97137   |\n",
      "| time_elapsed            | 836         |\n",
      "| total timesteps         | 45500       |\n",
      "| value_loss              | 203.54251   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.40956938  |\n",
      "| ent_coef_loss           | 0.047422234 |\n",
      "| entropy                 | -0.9012215  |\n",
      "| episodes                | 96          |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.75e+03   |\n",
      "| n_updates               | 47245       |\n",
      "| policy_loss             | 914.0355    |\n",
      "| qf1_loss                | 7062.762    |\n",
      "| qf2_loss                | 6986.582    |\n",
      "| time_elapsed            | 873         |\n",
      "| total timesteps         | 47500       |\n",
      "| value_loss              | 153.26054   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.4183468  |\n",
      "| ent_coef_loss           | 0.47677675 |\n",
      "| entropy                 | -0.6291277 |\n",
      "| episodes                | 100        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -6.7e+03   |\n",
      "| n_updates               | 49245      |\n",
      "| policy_loss             | 942.9916   |\n",
      "| qf1_loss                | 4159.155   |\n",
      "| qf2_loss                | 4043.158   |\n",
      "| time_elapsed            | 911        |\n",
      "| total timesteps         | 49500      |\n",
      "| value_loss              | 184.31488  |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.43578717   |\n",
      "| ent_coef_loss           | -0.059005357 |\n",
      "| entropy                 | -0.37053868  |\n",
      "| episodes                | 104          |\n",
      "| fps                     | 54           |\n",
      "| mean 100 episode reward | -6.47e+03    |\n",
      "| n_updates               | 51245        |\n",
      "| policy_loss             | 895.0803     |\n",
      "| qf1_loss                | 350.18042    |\n",
      "| qf2_loss                | 363.07233    |\n",
      "| time_elapsed            | 946          |\n",
      "| total timesteps         | 51500        |\n",
      "| value_loss              | 160.10396    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.42736095  |\n",
      "| ent_coef_loss           | 0.09020437  |\n",
      "| entropy                 | -0.42376778 |\n",
      "| episodes                | 108         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.21e+03   |\n",
      "| n_updates               | 53245       |\n",
      "| policy_loss             | 934.26526   |\n",
      "| qf1_loss                | 358.02667   |\n",
      "| qf2_loss                | 361.85306   |\n",
      "| time_elapsed            | 982         |\n",
      "| total timesteps         | 53500       |\n",
      "| value_loss              | 221.03278   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.42133948  |\n",
      "| ent_coef_loss           | 0.11683529  |\n",
      "| entropy                 | -0.44411182 |\n",
      "| episodes                | 112         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -6.22e+03   |\n",
      "| n_updates               | 55245       |\n",
      "| policy_loss             | 838.5146    |\n",
      "| qf1_loss                | 351.75598   |\n",
      "| qf2_loss                | 338.51294   |\n",
      "| time_elapsed            | 1019        |\n",
      "| total timesteps         | 55500       |\n",
      "| value_loss              | 160.8292    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.39634886 |\n",
      "| ent_coef_loss           | 0.09275165 |\n",
      "| entropy                 | -0.6916664 |\n",
      "| episodes                | 116        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -6.05e+03  |\n",
      "| n_updates               | 57245      |\n",
      "| policy_loss             | 850.51086  |\n",
      "| qf1_loss                | 3226.3057  |\n",
      "| qf2_loss                | 3325.4458  |\n",
      "| time_elapsed            | 1056       |\n",
      "| total timesteps         | 57500      |\n",
      "| value_loss              | 161.46185  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.38391402 |\n",
      "| ent_coef_loss           | 0.3336342  |\n",
      "| entropy                 | -0.9707747 |\n",
      "| episodes                | 120        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -5.68e+03  |\n",
      "| n_updates               | 59245      |\n",
      "| policy_loss             | 852.8086   |\n",
      "| qf1_loss                | 993.22314  |\n",
      "| qf2_loss                | 1203.8013  |\n",
      "| time_elapsed            | 1093       |\n",
      "| total timesteps         | 59500      |\n",
      "| value_loss              | 148.99332  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.3874998  |\n",
      "| ent_coef_loss           | 0.15565649 |\n",
      "| entropy                 | -1.1154313 |\n",
      "| episodes                | 124        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -5.74e+03  |\n",
      "| n_updates               | 61245      |\n",
      "| policy_loss             | 822.91833  |\n",
      "| qf1_loss                | 470.70062  |\n",
      "| qf2_loss                | 428.5487   |\n",
      "| time_elapsed            | 1130       |\n",
      "| total timesteps         | 61500      |\n",
      "| value_loss              | 210.94223  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.39446202 |\n",
      "| ent_coef_loss           | 0.36788738 |\n",
      "| entropy                 | -1.1113698 |\n",
      "| episodes                | 128        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -5.79e+03  |\n",
      "| n_updates               | 63245      |\n",
      "| policy_loss             | 828.0869   |\n",
      "| qf1_loss                | 4787.751   |\n",
      "| qf2_loss                | 4878.058   |\n",
      "| time_elapsed            | 1169       |\n",
      "| total timesteps         | 63500      |\n",
      "| value_loss              | 364.40634  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.39623836  |\n",
      "| ent_coef_loss           | -0.24305588 |\n",
      "| entropy                 | -1.1309835  |\n",
      "| episodes                | 132         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -5.74e+03   |\n",
      "| n_updates               | 65245       |\n",
      "| policy_loss             | 941.90247   |\n",
      "| qf1_loss                | 442.55194   |\n",
      "| qf2_loss                | 461.75302   |\n",
      "| time_elapsed            | 1207        |\n",
      "| total timesteps         | 65500       |\n",
      "| value_loss              | 188.38507   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.40952155 |\n",
      "| ent_coef_loss           | 0.17272197 |\n",
      "| entropy                 | -0.9958081 |\n",
      "| episodes                | 136        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -5.45e+03  |\n",
      "| n_updates               | 67245      |\n",
      "| policy_loss             | 911.9996   |\n",
      "| qf1_loss                | 421.29932  |\n",
      "| qf2_loss                | 410.53732  |\n",
      "| time_elapsed            | 1243       |\n",
      "| total timesteps         | 67500      |\n",
      "| value_loss              | 160.61334  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.4116013   |\n",
      "| ent_coef_loss           | -0.19372863 |\n",
      "| entropy                 | -0.87797123 |\n",
      "| episodes                | 140         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -5.27e+03   |\n",
      "| n_updates               | 69245       |\n",
      "| policy_loss             | 958.7765    |\n",
      "| qf1_loss                | 385.9517    |\n",
      "| qf2_loss                | 340.48856   |\n",
      "| time_elapsed            | 1280        |\n",
      "| total timesteps         | 69500       |\n",
      "| value_loss              | 245.01611   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.40484315 |\n",
      "| ent_coef_loss           | 0.12657104 |\n",
      "| entropy                 | -1.292045  |\n",
      "| episodes                | 144        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -4.77e+03  |\n",
      "| n_updates               | 71245      |\n",
      "| policy_loss             | 881.0622   |\n",
      "| qf1_loss                | 8029.0396  |\n",
      "| qf2_loss                | 7977.5654  |\n",
      "| time_elapsed            | 1314       |\n",
      "| total timesteps         | 71500      |\n",
      "| value_loss              | 191.77971  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.41183063 |\n",
      "| ent_coef_loss           | 0.33646122 |\n",
      "| entropy                 | -1.5428288 |\n",
      "| episodes                | 148        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -4.6e+03   |\n",
      "| n_updates               | 73245      |\n",
      "| policy_loss             | 881.88354  |\n",
      "| qf1_loss                | 463.31213  |\n",
      "| qf2_loss                | 449.34848  |\n",
      "| time_elapsed            | 1351       |\n",
      "| total timesteps         | 73500      |\n",
      "| value_loss              | 176.83156  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.42096388 |\n",
      "| ent_coef_loss           | 0.5167121  |\n",
      "| entropy                 | -1.1582913 |\n",
      "| episodes                | 152        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -4.32e+03  |\n",
      "| n_updates               | 75245      |\n",
      "| policy_loss             | 902.8809   |\n",
      "| qf1_loss                | 435.38116  |\n",
      "| qf2_loss                | 438.82608  |\n",
      "| time_elapsed            | 1387       |\n",
      "| total timesteps         | 75500      |\n",
      "| value_loss              | 171.51787  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.41751426 |\n",
      "| ent_coef_loss           | 0.10387097 |\n",
      "| entropy                 | -0.7775912 |\n",
      "| episodes                | 156        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -4.03e+03  |\n",
      "| n_updates               | 77245      |\n",
      "| policy_loss             | 814.87335  |\n",
      "| qf1_loss                | 412.79102  |\n",
      "| qf2_loss                | 444.28336  |\n",
      "| time_elapsed            | 1423       |\n",
      "| total timesteps         | 77500      |\n",
      "| value_loss              | 171.35388  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.4230882   |\n",
      "| ent_coef_loss           | -0.16408864 |\n",
      "| entropy                 | -1.3784026  |\n",
      "| episodes                | 160         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -4.1e+03    |\n",
      "| n_updates               | 79245       |\n",
      "| policy_loss             | 834.3827    |\n",
      "| qf1_loss                | 3168.1145   |\n",
      "| qf2_loss                | 3229.3884   |\n",
      "| time_elapsed            | 1460        |\n",
      "| total timesteps         | 79500       |\n",
      "| value_loss              | 792.74994   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.43325123  |\n",
      "| ent_coef_loss           | 0.08815093  |\n",
      "| entropy                 | -0.99974597 |\n",
      "| episodes                | 164         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -4.18e+03   |\n",
      "| n_updates               | 81245       |\n",
      "| policy_loss             | 913.6904    |\n",
      "| qf1_loss                | 1147.6494   |\n",
      "| qf2_loss                | 1138.446    |\n",
      "| time_elapsed            | 1496        |\n",
      "| total timesteps         | 81500       |\n",
      "| value_loss              | 291.25137   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.4477213   |\n",
      "| ent_coef_loss           | -0.04379525 |\n",
      "| entropy                 | -0.71785754 |\n",
      "| episodes                | 168         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -3.77e+03   |\n",
      "| n_updates               | 83245       |\n",
      "| policy_loss             | 818.79504   |\n",
      "| qf1_loss                | 399.22778   |\n",
      "| qf2_loss                | 363.9121    |\n",
      "| time_elapsed            | 1535        |\n",
      "| total timesteps         | 83500       |\n",
      "| value_loss              | 185.0291    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.46888494 |\n",
      "| ent_coef_loss           | 0.23926923 |\n",
      "| entropy                 | -0.418059  |\n",
      "| episodes                | 172        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -3.82e+03  |\n",
      "| n_updates               | 85245      |\n",
      "| policy_loss             | 881.69965  |\n",
      "| qf1_loss                | 406.34436  |\n",
      "| qf2_loss                | 362.98605  |\n",
      "| time_elapsed            | 1572       |\n",
      "| total timesteps         | 85500      |\n",
      "| value_loss              | 143.21878  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.4865767   |\n",
      "| ent_coef_loss           | 0.4125954   |\n",
      "| entropy                 | -0.14109021 |\n",
      "| episodes                | 176         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -3.36e+03   |\n",
      "| n_updates               | 87245       |\n",
      "| policy_loss             | 849.6953    |\n",
      "| qf1_loss                | 531.54034   |\n",
      "| qf2_loss                | 520.94946   |\n",
      "| time_elapsed            | 1609        |\n",
      "| total timesteps         | 87500       |\n",
      "| value_loss              | 240.49667   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.49997067  |\n",
      "| ent_coef_loss           | 0.008145098 |\n",
      "| entropy                 | -0.16509217 |\n",
      "| episodes                | 180         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -2.93e+03   |\n",
      "| n_updates               | 89245       |\n",
      "| policy_loss             | 834.45245   |\n",
      "| qf1_loss                | 963.6219    |\n",
      "| qf2_loss                | 728.5824    |\n",
      "| time_elapsed            | 1650        |\n",
      "| total timesteps         | 89500       |\n",
      "| value_loss              | 287.87073   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.5095084    |\n",
      "| ent_coef_loss           | -0.062659964 |\n",
      "| entropy                 | -0.149992    |\n",
      "| episodes                | 184          |\n",
      "| fps                     | 54           |\n",
      "| mean 100 episode reward | -2.68e+03    |\n",
      "| n_updates               | 91245        |\n",
      "| policy_loss             | 776.6673     |\n",
      "| qf1_loss                | 518.39435    |\n",
      "| qf2_loss                | 524.1921     |\n",
      "| time_elapsed            | 1690         |\n",
      "| total timesteps         | 91500        |\n",
      "| value_loss              | 260.6635     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5094431   |\n",
      "| ent_coef_loss           | -0.08146095 |\n",
      "| entropy                 | -0.1611181  |\n",
      "| episodes                | 188         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -2.43e+03   |\n",
      "| n_updates               | 93245       |\n",
      "| policy_loss             | 775.0671    |\n",
      "| qf1_loss                | 686.28485   |\n",
      "| qf2_loss                | 613.0669    |\n",
      "| time_elapsed            | 1728        |\n",
      "| total timesteps         | 93500       |\n",
      "| value_loss              | 248.63058   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.52961725  |\n",
      "| ent_coef_loss           | -0.16806008 |\n",
      "| entropy                 | 0.43489197  |\n",
      "| episodes                | 192         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -2.3e+03    |\n",
      "| n_updates               | 95245       |\n",
      "| policy_loss             | 748.80365   |\n",
      "| qf1_loss                | 964.2282    |\n",
      "| qf2_loss                | 948.32574   |\n",
      "| time_elapsed            | 1765        |\n",
      "| total timesteps         | 95500       |\n",
      "| value_loss              | 253.56998   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5416694  |\n",
      "| ent_coef_loss           | 0.1384998  |\n",
      "| entropy                 | 0.43710607 |\n",
      "| episodes                | 196        |\n",
      "| fps                     | 54         |\n",
      "| mean 100 episode reward | -2.23e+03  |\n",
      "| n_updates               | 97245      |\n",
      "| policy_loss             | 788.3347   |\n",
      "| qf1_loss                | 482.50366  |\n",
      "| qf2_loss                | 537.0327   |\n",
      "| time_elapsed            | 1802       |\n",
      "| total timesteps         | 97500      |\n",
      "| value_loss              | 255.47081  |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.5473567    |\n",
      "| ent_coef_loss           | -0.019620225 |\n",
      "| entropy                 | 0.31792152   |\n",
      "| episodes                | 200          |\n",
      "| fps                     | 54           |\n",
      "| mean 100 episode reward | -2.09e+03    |\n",
      "| n_updates               | 99245        |\n",
      "| policy_loss             | 798.9943     |\n",
      "| qf1_loss                | 738.8976     |\n",
      "| qf2_loss                | 766.75806    |\n",
      "| time_elapsed            | 1837         |\n",
      "| total timesteps         | 99500        |\n",
      "| value_loss              | 220.85245    |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.5601282    |\n",
      "| ent_coef_loss           | -0.101120494 |\n",
      "| entropy                 | 0.39047292   |\n",
      "| episodes                | 204          |\n",
      "| fps                     | 54           |\n",
      "| mean 100 episode reward | -1.96e+03    |\n",
      "| n_updates               | 101245       |\n",
      "| policy_loss             | 684.66565    |\n",
      "| qf1_loss                | 682.4923     |\n",
      "| qf2_loss                | 638.0969     |\n",
      "| time_elapsed            | 1874         |\n",
      "| total timesteps         | 101500       |\n",
      "| value_loss              | 247.99559    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5536021   |\n",
      "| ent_coef_loss           | 0.046687067 |\n",
      "| entropy                 | -0.33684567 |\n",
      "| episodes                | 208         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -1.95e+03   |\n",
      "| n_updates               | 103245      |\n",
      "| policy_loss             | 691.093     |\n",
      "| qf1_loss                | 3417.3691   |\n",
      "| qf2_loss                | 3505.9607   |\n",
      "| time_elapsed            | 1911        |\n",
      "| total timesteps         | 103500      |\n",
      "| value_loss              | 335.26218   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5632252   |\n",
      "| ent_coef_loss           | 0.09008793  |\n",
      "| entropy                 | 0.102183454 |\n",
      "| episodes                | 212         |\n",
      "| fps                     | 54          |\n",
      "| mean 100 episode reward | -1.86e+03   |\n",
      "| n_updates               | 105245      |\n",
      "| policy_loss             | 740.84143   |\n",
      "| qf1_loss                | 710.6457    |\n",
      "| qf2_loss                | 686.4259    |\n",
      "| time_elapsed            | 1952        |\n",
      "| total timesteps         | 105500      |\n",
      "| value_loss              | 249.09048   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.555286    |\n",
      "| ent_coef_loss           | -0.07331935 |\n",
      "| entropy                 | 0.043389965 |\n",
      "| episodes                | 216         |\n",
      "| fps                     | 53          |\n",
      "| mean 100 episode reward | -1.73e+03   |\n",
      "| n_updates               | 107245      |\n",
      "| policy_loss             | 698.5244    |\n",
      "| qf1_loss                | 518.30756   |\n",
      "| qf2_loss                | 538.1015    |\n",
      "| time_elapsed            | 1990        |\n",
      "| total timesteps         | 107500      |\n",
      "| value_loss              | 255.19548   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5589639  |\n",
      "| ent_coef_loss           | 0.10642205 |\n",
      "| entropy                 | 0.11426163 |\n",
      "| episodes                | 220        |\n",
      "| fps                     | 53         |\n",
      "| mean 100 episode reward | -1.88e+03  |\n",
      "| n_updates               | 109245     |\n",
      "| policy_loss             | 640.48267  |\n",
      "| qf1_loss                | 856.168    |\n",
      "| qf2_loss                | 837.2641   |\n",
      "| time_elapsed            | 2031       |\n",
      "| total timesteps         | 109500     |\n",
      "| value_loss              | 427.64368  |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.55500513   |\n",
      "| ent_coef_loss           | -0.015055997 |\n",
      "| entropy                 | 0.32800174   |\n",
      "| episodes                | 224          |\n",
      "| fps                     | 53           |\n",
      "| mean 100 episode reward | -1.89e+03    |\n",
      "| n_updates               | 111245       |\n",
      "| policy_loss             | 723.80493    |\n",
      "| qf1_loss                | 698.5568     |\n",
      "| qf2_loss                | 612.98566    |\n",
      "| time_elapsed            | 2072         |\n",
      "| total timesteps         | 111500       |\n",
      "| value_loss              | 250.84985    |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5690748  |\n",
      "| ent_coef_loss           | 0.22355947 |\n",
      "| entropy                 | 0.23579618 |\n",
      "| episodes                | 228        |\n",
      "| fps                     | 53         |\n",
      "| mean 100 episode reward | -1.71e+03  |\n",
      "| n_updates               | 113245     |\n",
      "| policy_loss             | 586.8726   |\n",
      "| qf1_loss                | 1351.4916  |\n",
      "| qf2_loss                | 1441.9563  |\n",
      "| time_elapsed            | 2113       |\n",
      "| total timesteps         | 113500     |\n",
      "| value_loss              | 245.97897  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.56151694  |\n",
      "| ent_coef_loss           | 0.35292086  |\n",
      "| entropy                 | -0.14170241 |\n",
      "| episodes                | 232         |\n",
      "| fps                     | 53          |\n",
      "| mean 100 episode reward | -1.69e+03   |\n",
      "| n_updates               | 115245      |\n",
      "| policy_loss             | 712.6808    |\n",
      "| qf1_loss                | 522.242     |\n",
      "| qf2_loss                | 533.4419    |\n",
      "| time_elapsed            | 2153        |\n",
      "| total timesteps         | 115500      |\n",
      "| value_loss              | 234.63129   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5572587   |\n",
      "| ent_coef_loss           | 0.43840882  |\n",
      "| entropy                 | -0.04865633 |\n",
      "| episodes                | 236         |\n",
      "| fps                     | 53          |\n",
      "| mean 100 episode reward | -1.45e+03   |\n",
      "| n_updates               | 117245      |\n",
      "| policy_loss             | 648.84753   |\n",
      "| qf1_loss                | 633.46515   |\n",
      "| qf2_loss                | 690.1157    |\n",
      "| time_elapsed            | 2193        |\n",
      "| total timesteps         | 117500      |\n",
      "| value_loss              | 242.16248   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.55594283 |\n",
      "| ent_coef_loss           | 0.14618847 |\n",
      "| entropy                 | 0.15815336 |\n",
      "| episodes                | 240        |\n",
      "| fps                     | 53         |\n",
      "| mean 100 episode reward | -1.59e+03  |\n",
      "| n_updates               | 119245     |\n",
      "| policy_loss             | 708.9033   |\n",
      "| qf1_loss                | 778.72864  |\n",
      "| qf2_loss                | 651.01294  |\n",
      "| time_elapsed            | 2234       |\n",
      "| total timesteps         | 119500     |\n",
      "| value_loss              | 314.09283  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5440056   |\n",
      "| ent_coef_loss           | -0.17263195 |\n",
      "| entropy                 | 0.35158467  |\n",
      "| episodes                | 244         |\n",
      "| fps                     | 53          |\n",
      "| mean 100 episode reward | -1.65e+03   |\n",
      "| n_updates               | 121245      |\n",
      "| policy_loss             | 617.83496   |\n",
      "| qf1_loss                | 552.27295   |\n",
      "| qf2_loss                | 581.6526    |\n",
      "| time_elapsed            | 2273        |\n",
      "| total timesteps         | 121500      |\n",
      "| value_loss              | 188.20247   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.55098724   |\n",
      "| ent_coef_loss           | -0.123533905 |\n",
      "| entropy                 | -0.19528188  |\n",
      "| episodes                | 248          |\n",
      "| fps                     | 53           |\n",
      "| mean 100 episode reward | -1.69e+03    |\n",
      "| n_updates               | 123245       |\n",
      "| policy_loss             | 667.6633     |\n",
      "| qf1_loss                | 5595.3433    |\n",
      "| qf2_loss                | 5636.215     |\n",
      "| time_elapsed            | 2313         |\n",
      "| total timesteps         | 123500       |\n",
      "| value_loss              | 284.2976     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5502297   |\n",
      "| ent_coef_loss           | -0.20567243 |\n",
      "| entropy                 | 0.02888662  |\n",
      "| episodes                | 252         |\n",
      "| fps                     | 53          |\n",
      "| mean 100 episode reward | -1.83e+03   |\n",
      "| n_updates               | 125245      |\n",
      "| policy_loss             | 630.9806    |\n",
      "| qf1_loss                | 622.4911    |\n",
      "| qf2_loss                | 627.6403    |\n",
      "| time_elapsed            | 2354        |\n",
      "| total timesteps         | 125500      |\n",
      "| value_loss              | 252.31845   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5479638  |\n",
      "| ent_coef_loss           | -0.2920652 |\n",
      "| entropy                 | 0.26105168 |\n",
      "| episodes                | 256        |\n",
      "| fps                     | 53         |\n",
      "| mean 100 episode reward | -1.99e+03  |\n",
      "| n_updates               | 127245     |\n",
      "| policy_loss             | 594.04456  |\n",
      "| qf1_loss                | 737.2414   |\n",
      "| qf2_loss                | 740.24445  |\n",
      "| time_elapsed            | 2395       |\n",
      "| total timesteps         | 127500     |\n",
      "| value_loss              | 196.98566  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5575025  |\n",
      "| ent_coef_loss           | 0.15696365 |\n",
      "| entropy                 | 0.5177911  |\n",
      "| episodes                | 260        |\n",
      "| fps                     | 53         |\n",
      "| mean 100 episode reward | -2.01e+03  |\n",
      "| n_updates               | 129245     |\n",
      "| policy_loss             | 570.98456  |\n",
      "| qf1_loss                | 2646.373   |\n",
      "| qf2_loss                | 2542.5068  |\n",
      "| time_elapsed            | 2435       |\n",
      "| total timesteps         | 129500     |\n",
      "| value_loss              | 166.62723  |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.5815789    |\n",
      "| ent_coef_loss           | -0.24443127  |\n",
      "| entropy                 | -0.020690449 |\n",
      "| episodes                | 264          |\n",
      "| fps                     | 53           |\n",
      "| mean 100 episode reward | -1.96e+03    |\n",
      "| n_updates               | 131245       |\n",
      "| policy_loss             | 700.73206    |\n",
      "| qf1_loss                | 570.116      |\n",
      "| qf2_loss                | 626.9472     |\n",
      "| time_elapsed            | 2477         |\n",
      "| total timesteps         | 131500       |\n",
      "| value_loss              | 271.4306     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.56720877  |\n",
      "| ent_coef_loss           | -0.24345979 |\n",
      "| entropy                 | 0.1704606   |\n",
      "| episodes                | 268         |\n",
      "| fps                     | 53          |\n",
      "| mean 100 episode reward | -2.15e+03   |\n",
      "| n_updates               | 133245      |\n",
      "| policy_loss             | 564.45325   |\n",
      "| qf1_loss                | 3454.1848   |\n",
      "| qf2_loss                | 2639.6455   |\n",
      "| time_elapsed            | 2518        |\n",
      "| total timesteps         | 133500      |\n",
      "| value_loss              | 376.161     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5638846   |\n",
      "| ent_coef_loss           | 0.13012266  |\n",
      "| entropy                 | -0.11731297 |\n",
      "| episodes                | 272         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -2.07e+03   |\n",
      "| n_updates               | 135245      |\n",
      "| policy_loss             | 613.1301    |\n",
      "| qf1_loss                | 542.8013    |\n",
      "| qf2_loss                | 495.47827   |\n",
      "| time_elapsed            | 2560        |\n",
      "| total timesteps         | 135500      |\n",
      "| value_loss              | 275.73358   |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0001        |\n",
      "| ent_coef                | 0.5551499     |\n",
      "| ent_coef_loss           | -0.0031407923 |\n",
      "| entropy                 | 0.2612018     |\n",
      "| episodes                | 276           |\n",
      "| fps                     | 52            |\n",
      "| mean 100 episode reward | -2.48e+03     |\n",
      "| n_updates               | 137245        |\n",
      "| policy_loss             | 625.08405     |\n",
      "| qf1_loss                | 527.775       |\n",
      "| qf2_loss                | 608.9959      |\n",
      "| time_elapsed            | 2602          |\n",
      "| total timesteps         | 137500        |\n",
      "| value_loss              | 327.0428      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.55730116   |\n",
      "| ent_coef_loss           | -0.029281057 |\n",
      "| entropy                 | 0.08778775   |\n",
      "| episodes                | 280          |\n",
      "| fps                     | 52           |\n",
      "| mean 100 episode reward | -2.55e+03    |\n",
      "| n_updates               | 139245       |\n",
      "| policy_loss             | 644.9061     |\n",
      "| qf1_loss                | 8604.052     |\n",
      "| qf2_loss                | 8864.028     |\n",
      "| time_elapsed            | 2643         |\n",
      "| total timesteps         | 139500       |\n",
      "| value_loss              | 321.22382    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.57250917  |\n",
      "| ent_coef_loss           | 0.028821573 |\n",
      "| entropy                 | 0.26934958  |\n",
      "| episodes                | 284         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -2.78e+03   |\n",
      "| n_updates               | 141245      |\n",
      "| policy_loss             | 622.0502    |\n",
      "| qf1_loss                | 3305.2676   |\n",
      "| qf2_loss                | 3242.6357   |\n",
      "| time_elapsed            | 2685        |\n",
      "| total timesteps         | 141500      |\n",
      "| value_loss              | 331.6507    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.57420087 |\n",
      "| ent_coef_loss           | 0.1774312  |\n",
      "| entropy                 | 0.28002122 |\n",
      "| episodes                | 288        |\n",
      "| fps                     | 52         |\n",
      "| mean 100 episode reward | -2.76e+03  |\n",
      "| n_updates               | 143245     |\n",
      "| policy_loss             | 768.07227  |\n",
      "| qf1_loss                | 2182.386   |\n",
      "| qf2_loss                | 2182.9329  |\n",
      "| time_elapsed            | 2725       |\n",
      "| total timesteps         | 143500     |\n",
      "| value_loss              | 210.71725  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5627744   |\n",
      "| ent_coef_loss           | 0.006954087 |\n",
      "| entropy                 | 0.25528538  |\n",
      "| episodes                | 292         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -2.91e+03   |\n",
      "| n_updates               | 145245      |\n",
      "| policy_loss             | 586.55286   |\n",
      "| qf1_loss                | 3253.1096   |\n",
      "| qf2_loss                | 3829.9453   |\n",
      "| time_elapsed            | 2767        |\n",
      "| total timesteps         | 145500      |\n",
      "| value_loss              | 316.0385    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.57136637  |\n",
      "| ent_coef_loss           | -0.03263098 |\n",
      "| entropy                 | 0.12547903  |\n",
      "| episodes                | 296         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -2.68e+03   |\n",
      "| n_updates               | 147245      |\n",
      "| policy_loss             | 693.1424    |\n",
      "| qf1_loss                | 471.95947   |\n",
      "| qf2_loss                | 542.67145   |\n",
      "| time_elapsed            | 2806        |\n",
      "| total timesteps         | 147500      |\n",
      "| value_loss              | 247.83395   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5843177  |\n",
      "| ent_coef_loss           | 0.11555414 |\n",
      "| entropy                 | 0.3665     |\n",
      "| episodes                | 300        |\n",
      "| fps                     | 52         |\n",
      "| mean 100 episode reward | -2.76e+03  |\n",
      "| n_updates               | 149245     |\n",
      "| policy_loss             | 667.2623   |\n",
      "| qf1_loss                | 751.0692   |\n",
      "| qf2_loss                | 726.516    |\n",
      "| time_elapsed            | 2849       |\n",
      "| total timesteps         | 149500     |\n",
      "| value_loss              | 234.56505  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5989761   |\n",
      "| ent_coef_loss           | -0.05441732 |\n",
      "| entropy                 | 0.70537096  |\n",
      "| episodes                | 304         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -2.99e+03   |\n",
      "| n_updates               | 151245      |\n",
      "| policy_loss             | 691.3179    |\n",
      "| qf1_loss                | 960.86053   |\n",
      "| qf2_loss                | 1001.0829   |\n",
      "| time_elapsed            | 2890        |\n",
      "| total timesteps         | 151500      |\n",
      "| value_loss              | 202.02652   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.62493736  |\n",
      "| ent_coef_loss           | -0.18156348 |\n",
      "| entropy                 | 1.2947137   |\n",
      "| episodes                | 308         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -3.21e+03   |\n",
      "| n_updates               | 153245      |\n",
      "| policy_loss             | 593.25183   |\n",
      "| qf1_loss                | 467.20618   |\n",
      "| qf2_loss                | 448.2479    |\n",
      "| time_elapsed            | 2933        |\n",
      "| total timesteps         | 153500      |\n",
      "| value_loss              | 195.59995   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6405359   |\n",
      "| ent_coef_loss           | 0.051783722 |\n",
      "| entropy                 | 1.083287    |\n",
      "| episodes                | 312         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -3.27e+03   |\n",
      "| n_updates               | 155245      |\n",
      "| policy_loss             | 725.67334   |\n",
      "| qf1_loss                | 381.7042    |\n",
      "| qf2_loss                | 456.79626   |\n",
      "| time_elapsed            | 2976        |\n",
      "| total timesteps         | 155500      |\n",
      "| value_loss              | 155.19794   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.64875364 |\n",
      "| ent_coef_loss           | 0.17566445 |\n",
      "| entropy                 | 0.9528588  |\n",
      "| episodes                | 316        |\n",
      "| fps                     | 52         |\n",
      "| mean 100 episode reward | -3.21e+03  |\n",
      "| n_updates               | 157245     |\n",
      "| policy_loss             | 666.50336  |\n",
      "| qf1_loss                | 6164.288   |\n",
      "| qf2_loss                | 6342.805   |\n",
      "| time_elapsed            | 3017       |\n",
      "| total timesteps         | 157500     |\n",
      "| value_loss              | 234.13591  |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| current_lr              | 0.0001    |\n",
      "| ent_coef                | 0.6475312 |\n",
      "| ent_coef_loss           | 0.3102656 |\n",
      "| entropy                 | 1.1465471 |\n",
      "| episodes                | 320       |\n",
      "| fps                     | 52        |\n",
      "| mean 100 episode reward | -3.34e+03 |\n",
      "| n_updates               | 159245    |\n",
      "| policy_loss             | 678.7148  |\n",
      "| qf1_loss                | 589.3428  |\n",
      "| qf2_loss                | 539.4256  |\n",
      "| time_elapsed            | 3060      |\n",
      "| total timesteps         | 159500    |\n",
      "| value_loss              | 176.81482 |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6198501   |\n",
      "| ent_coef_loss           | 0.025154259 |\n",
      "| entropy                 | 0.636619    |\n",
      "| episodes                | 324         |\n",
      "| fps                     | 52          |\n",
      "| mean 100 episode reward | -3.1e+03    |\n",
      "| n_updates               | 161245      |\n",
      "| policy_loss             | 822.28284   |\n",
      "| qf1_loss                | 773.3327    |\n",
      "| qf2_loss                | 699.0466    |\n",
      "| time_elapsed            | 3099        |\n",
      "| total timesteps         | 161500      |\n",
      "| value_loss              | 239.6288    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5973381  |\n",
      "| ent_coef_loss           | 0.14862144 |\n",
      "| entropy                 | 0.31642652 |\n",
      "| episodes                | 328        |\n",
      "| fps                     | 52         |\n",
      "| mean 100 episode reward | -3.18e+03  |\n",
      "| n_updates               | 163245     |\n",
      "| policy_loss             | 794.1358   |\n",
      "| qf1_loss                | 912.8868   |\n",
      "| qf2_loss                | 806.15717  |\n",
      "| time_elapsed            | 3142       |\n",
      "| total timesteps         | 163500     |\n",
      "| value_loss              | 258.13834  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5846305  |\n",
      "| ent_coef_loss           | 0.13864246 |\n",
      "| entropy                 | 0.37747848 |\n",
      "| episodes                | 332        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -3.1e+03   |\n",
      "| n_updates               | 165245     |\n",
      "| policy_loss             | 685.1598   |\n",
      "| qf1_loss                | 1097.6292  |\n",
      "| qf2_loss                | 972.44226  |\n",
      "| time_elapsed            | 3183       |\n",
      "| total timesteps         | 165500     |\n",
      "| value_loss              | 300.0294   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5844269   |\n",
      "| ent_coef_loss           | 0.005806815 |\n",
      "| entropy                 | 0.37989503  |\n",
      "| episodes                | 336         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -3.28e+03   |\n",
      "| n_updates               | 167245      |\n",
      "| policy_loss             | 640.34845   |\n",
      "| qf1_loss                | 6064.868    |\n",
      "| qf2_loss                | 6123.7393   |\n",
      "| time_elapsed            | 3223        |\n",
      "| total timesteps         | 167500      |\n",
      "| value_loss              | 193.77249   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.59910715 |\n",
      "| ent_coef_loss           | 0.07247907 |\n",
      "| entropy                 | 0.44125643 |\n",
      "| episodes                | 340        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -3.1e+03   |\n",
      "| n_updates               | 169245     |\n",
      "| policy_loss             | 529.7551   |\n",
      "| qf1_loss                | 434.3972   |\n",
      "| qf2_loss                | 369.6892   |\n",
      "| time_elapsed            | 3264       |\n",
      "| total timesteps         | 169500     |\n",
      "| value_loss              | 156.48651  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6181373   |\n",
      "| ent_coef_loss           | 0.039108925 |\n",
      "| entropy                 | 0.5632256   |\n",
      "| episodes                | 344         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -3.31e+03   |\n",
      "| n_updates               | 171245      |\n",
      "| policy_loss             | 672.09454   |\n",
      "| qf1_loss                | 704.04016   |\n",
      "| qf2_loss                | 674.1212    |\n",
      "| time_elapsed            | 3308        |\n",
      "| total timesteps         | 171500      |\n",
      "| value_loss              | 201.232     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.6419813  |\n",
      "| ent_coef_loss           | 0.11593656 |\n",
      "| entropy                 | 0.7950021  |\n",
      "| episodes                | 348        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -3.26e+03  |\n",
      "| n_updates               | 173245     |\n",
      "| policy_loss             | 610.8882   |\n",
      "| qf1_loss                | 521.36383  |\n",
      "| qf2_loss                | 541.08496  |\n",
      "| time_elapsed            | 3350       |\n",
      "| total timesteps         | 173500     |\n",
      "| value_loss              | 246.46518  |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.6394863    |\n",
      "| ent_coef_loss           | -0.102486566 |\n",
      "| entropy                 | 0.72080946   |\n",
      "| episodes                | 352          |\n",
      "| fps                     | 51           |\n",
      "| mean 100 episode reward | -3.18e+03    |\n",
      "| n_updates               | 175245       |\n",
      "| policy_loss             | 643.0522     |\n",
      "| qf1_loss                | 460.19366    |\n",
      "| qf2_loss                | 534.89825    |\n",
      "| time_elapsed            | 3391         |\n",
      "| total timesteps         | 175500       |\n",
      "| value_loss              | 286.89746    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6256519   |\n",
      "| ent_coef_loss           | -0.26807487 |\n",
      "| entropy                 | 0.81590456  |\n",
      "| episodes                | 356         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -3.12e+03   |\n",
      "| n_updates               | 177245      |\n",
      "| policy_loss             | 615.79535   |\n",
      "| qf1_loss                | 503.745     |\n",
      "| qf2_loss                | 425.46667   |\n",
      "| time_elapsed            | 3433        |\n",
      "| total timesteps         | 177500      |\n",
      "| value_loss              | 203.018     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6175425   |\n",
      "| ent_coef_loss           | -0.19585411 |\n",
      "| entropy                 | 0.46482325  |\n",
      "| episodes                | 360         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -2.83e+03   |\n",
      "| n_updates               | 179245      |\n",
      "| policy_loss             | 615.35034   |\n",
      "| qf1_loss                | 546.1274    |\n",
      "| qf2_loss                | 576.3906    |\n",
      "| time_elapsed            | 3476        |\n",
      "| total timesteps         | 179500      |\n",
      "| value_loss              | 216.46837   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.6052241    |\n",
      "| ent_coef_loss           | -0.043460563 |\n",
      "| entropy                 | 0.49763227   |\n",
      "| episodes                | 364          |\n",
      "| fps                     | 51           |\n",
      "| mean 100 episode reward | -2.91e+03    |\n",
      "| n_updates               | 181245       |\n",
      "| policy_loss             | 704.67725    |\n",
      "| qf1_loss                | 2501.5671    |\n",
      "| qf2_loss                | 2651.893     |\n",
      "| time_elapsed            | 3518         |\n",
      "| total timesteps         | 181500       |\n",
      "| value_loss              | 540.57135    |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.60410386 |\n",
      "| ent_coef_loss           | 0.2589188  |\n",
      "| entropy                 | 0.44384006 |\n",
      "| episodes                | 368        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -3.2e+03   |\n",
      "| n_updates               | 183245     |\n",
      "| policy_loss             | 613.223    |\n",
      "| qf1_loss                | 639.88025  |\n",
      "| qf2_loss                | 552.65625  |\n",
      "| time_elapsed            | 3560       |\n",
      "| total timesteps         | 183500     |\n",
      "| value_loss              | 274.759    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.61411697 |\n",
      "| ent_coef_loss           | 0.16662021 |\n",
      "| entropy                 | 0.51997995 |\n",
      "| episodes                | 372        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -2.93e+03  |\n",
      "| n_updates               | 185245     |\n",
      "| policy_loss             | 634.7312   |\n",
      "| qf1_loss                | 501.08273  |\n",
      "| qf2_loss                | 443.36435  |\n",
      "| time_elapsed            | 3601       |\n",
      "| total timesteps         | 185500     |\n",
      "| value_loss              | 260.88675  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5988082  |\n",
      "| ent_coef_loss           | -0.0234298 |\n",
      "| entropy                 | 0.0954043  |\n",
      "| episodes                | 376        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -2.77e+03  |\n",
      "| n_updates               | 187245     |\n",
      "| policy_loss             | 612.32495  |\n",
      "| qf1_loss                | 644.8606   |\n",
      "| qf2_loss                | 582.2041   |\n",
      "| time_elapsed            | 3642       |\n",
      "| total timesteps         | 187500     |\n",
      "| value_loss              | 195.24669  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.60621613  |\n",
      "| ent_coef_loss           | -0.06807906 |\n",
      "| entropy                 | 0.12435636  |\n",
      "| episodes                | 380         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -3.04e+03   |\n",
      "| n_updates               | 189245      |\n",
      "| policy_loss             | 553.13385   |\n",
      "| qf1_loss                | 3448.667    |\n",
      "| qf2_loss                | 3155.1506   |\n",
      "| time_elapsed            | 3684        |\n",
      "| total timesteps         | 189500      |\n",
      "| value_loss              | 309.02396   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.6030743  |\n",
      "| ent_coef_loss           | 0.07019774 |\n",
      "| entropy                 | 0.1811203  |\n",
      "| episodes                | 384        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -2.97e+03  |\n",
      "| n_updates               | 191245     |\n",
      "| policy_loss             | 749.63446  |\n",
      "| qf1_loss                | 395.84253  |\n",
      "| qf2_loss                | 420.0893   |\n",
      "| time_elapsed            | 3726       |\n",
      "| total timesteps         | 191500     |\n",
      "| value_loss              | 273.61356  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.6072039  |\n",
      "| ent_coef_loss           | 0.08715844 |\n",
      "| entropy                 | 0.23054682 |\n",
      "| episodes                | 388        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -3e+03     |\n",
      "| n_updates               | 193245     |\n",
      "| policy_loss             | 610.18243  |\n",
      "| qf1_loss                | 847.7329   |\n",
      "| qf2_loss                | 497.69382  |\n",
      "| time_elapsed            | 3768       |\n",
      "| total timesteps         | 193500     |\n",
      "| value_loss              | 260.8004   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.5988644  |\n",
      "| ent_coef_loss           | 0.15058208 |\n",
      "| entropy                 | -0.1597909 |\n",
      "| episodes                | 392        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -2.84e+03  |\n",
      "| n_updates               | 195245     |\n",
      "| policy_loss             | 638.5862   |\n",
      "| qf1_loss                | 4036.0522  |\n",
      "| qf2_loss                | 3993.4985  |\n",
      "| time_elapsed            | 3810       |\n",
      "| total timesteps         | 195500     |\n",
      "| value_loss              | 264.37482  |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.5937219    |\n",
      "| ent_coef_loss           | 0.025983717  |\n",
      "| entropy                 | -0.018296307 |\n",
      "| episodes                | 396          |\n",
      "| fps                     | 51           |\n",
      "| mean 100 episode reward | -2.9e+03     |\n",
      "| n_updates               | 197245       |\n",
      "| policy_loss             | 804.6488     |\n",
      "| qf1_loss                | 413.29498    |\n",
      "| qf2_loss                | 418.79407    |\n",
      "| time_elapsed            | 3851         |\n",
      "| total timesteps         | 197500       |\n",
      "| value_loss              | 190.93848    |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.6005416  |\n",
      "| ent_coef_loss           | 0.17345467 |\n",
      "| entropy                 | 0.1562781  |\n",
      "| episodes                | 400        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -2.99e+03  |\n",
      "| n_updates               | 199245     |\n",
      "| policy_loss             | 706.6565   |\n",
      "| qf1_loss                | 11488.18   |\n",
      "| qf2_loss                | 11380.288  |\n",
      "| time_elapsed            | 3894       |\n",
      "| total timesteps         | 199500     |\n",
      "| value_loss              | 177.20192  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.58867836  |\n",
      "| ent_coef_loss           | -0.29590476 |\n",
      "| entropy                 | -0.08843236 |\n",
      "| episodes                | 404         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -2.95e+03   |\n",
      "| n_updates               | 201245      |\n",
      "| policy_loss             | 668.97375   |\n",
      "| qf1_loss                | 723.7942    |\n",
      "| qf2_loss                | 763.4248    |\n",
      "| time_elapsed            | 3936        |\n",
      "| total timesteps         | 201500      |\n",
      "| value_loss              | 198.34726   |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.60649234   |\n",
      "| ent_coef_loss           | -0.010207169 |\n",
      "| entropy                 | 0.14242333   |\n",
      "| episodes                | 408          |\n",
      "| fps                     | 51           |\n",
      "| mean 100 episode reward | -2.99e+03    |\n",
      "| n_updates               | 203245       |\n",
      "| policy_loss             | 563.27997    |\n",
      "| qf1_loss                | 553.79083    |\n",
      "| qf2_loss                | 446.91342    |\n",
      "| time_elapsed            | 3980         |\n",
      "| total timesteps         | 203500       |\n",
      "| value_loss              | 266.8974     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6070054   |\n",
      "| ent_coef_loss           | 0.018091269 |\n",
      "| entropy                 | -0.13407376 |\n",
      "| episodes                | 412         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -2.57e+03   |\n",
      "| n_updates               | 205245      |\n",
      "| policy_loss             | 622.35333   |\n",
      "| qf1_loss                | 417.02722   |\n",
      "| qf2_loss                | 466.6726    |\n",
      "| time_elapsed            | 4021        |\n",
      "| total timesteps         | 205500      |\n",
      "| value_loss              | 200.70064   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.60005724  |\n",
      "| ent_coef_loss           | 0.072260216 |\n",
      "| entropy                 | 0.02778284  |\n",
      "| episodes                | 416         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -2.63e+03   |\n",
      "| n_updates               | 207245      |\n",
      "| policy_loss             | 691.34753   |\n",
      "| qf1_loss                | 6128.621    |\n",
      "| qf2_loss                | 6053.933    |\n",
      "| time_elapsed            | 4063        |\n",
      "| total timesteps         | 207500      |\n",
      "| value_loss              | 332.27686   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5946594   |\n",
      "| ent_coef_loss           | 0.15737064  |\n",
      "| entropy                 | -0.37164629 |\n",
      "| episodes                | 420         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -2.3e+03    |\n",
      "| n_updates               | 209245      |\n",
      "| policy_loss             | 769.56714   |\n",
      "| qf1_loss                | 369.29132   |\n",
      "| qf2_loss                | 335.6456    |\n",
      "| time_elapsed            | 4104        |\n",
      "| total timesteps         | 209500      |\n",
      "| value_loss              | 184.90027   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.58996147 |\n",
      "| ent_coef_loss           | 0.24057569 |\n",
      "| entropy                 | 0.06265022 |\n",
      "| episodes                | 424        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | -2.29e+03  |\n",
      "| n_updates               | 211245     |\n",
      "| policy_loss             | 748.4719   |\n",
      "| qf1_loss                | 414.1514   |\n",
      "| qf2_loss                | 475.99103  |\n",
      "| time_elapsed            | 4145       |\n",
      "| total timesteps         | 211500     |\n",
      "| value_loss              | 220.3721   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.58731836  |\n",
      "| ent_coef_loss           | -0.26852778 |\n",
      "| entropy                 | -0.26325193 |\n",
      "| episodes                | 428         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.13e+03   |\n",
      "| n_updates               | 213245      |\n",
      "| policy_loss             | 567.83167   |\n",
      "| qf1_loss                | 528.2446    |\n",
      "| qf2_loss                | 646.3815    |\n",
      "| time_elapsed            | 4187        |\n",
      "| total timesteps         | 213500      |\n",
      "| value_loss              | 295.49008   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.59661645  |\n",
      "| ent_coef_loss           | -0.09540062 |\n",
      "| entropy                 | -0.04209272 |\n",
      "| episodes                | 432         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.14e+03   |\n",
      "| n_updates               | 215245      |\n",
      "| policy_loss             | 647.5602    |\n",
      "| qf1_loss                | 449.24152   |\n",
      "| qf2_loss                | 472.12552   |\n",
      "| time_elapsed            | 4229        |\n",
      "| total timesteps         | 215500      |\n",
      "| value_loss              | 171.81427   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.579646    |\n",
      "| ent_coef_loss           | 0.24767865  |\n",
      "| entropy                 | -0.03852371 |\n",
      "| episodes                | 436         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.22e+03   |\n",
      "| n_updates               | 217245      |\n",
      "| policy_loss             | 755.18854   |\n",
      "| qf1_loss                | 5477.249    |\n",
      "| qf2_loss                | 5409.711    |\n",
      "| time_elapsed            | 4271        |\n",
      "| total timesteps         | 217500      |\n",
      "| value_loss              | 319.71924   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5775153   |\n",
      "| ent_coef_loss           | 0.32494408  |\n",
      "| entropy                 | -0.30600706 |\n",
      "| episodes                | 440         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.2e+03    |\n",
      "| n_updates               | 219245      |\n",
      "| policy_loss             | 594.0733    |\n",
      "| qf1_loss                | 1054.9246   |\n",
      "| qf2_loss                | 1022.9783   |\n",
      "| time_elapsed            | 4312        |\n",
      "| total timesteps         | 219500      |\n",
      "| value_loss              | 206.72102   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.59192705  |\n",
      "| ent_coef_loss           | -0.37274984 |\n",
      "| entropy                 | -0.35952672 |\n",
      "| episodes                | 444         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.14e+03   |\n",
      "| n_updates               | 221245      |\n",
      "| policy_loss             | 701.17993   |\n",
      "| qf1_loss                | 423.50195   |\n",
      "| qf2_loss                | 552.6373    |\n",
      "| time_elapsed            | 4355        |\n",
      "| total timesteps         | 221500      |\n",
      "| value_loss              | 201.6796    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5944781   |\n",
      "| ent_coef_loss           | 0.107584655 |\n",
      "| entropy                 | -0.19556403 |\n",
      "| episodes                | 448         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -1.91e+03   |\n",
      "| n_updates               | 223245      |\n",
      "| policy_loss             | 638.4619    |\n",
      "| qf1_loss                | 375.3205    |\n",
      "| qf2_loss                | 339.60034   |\n",
      "| time_elapsed            | 4394        |\n",
      "| total timesteps         | 223500      |\n",
      "| value_loss              | 186.70657   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.59435904  |\n",
      "| ent_coef_loss           | -0.04669565 |\n",
      "| entropy                 | -0.2533553  |\n",
      "| episodes                | 452         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -1.88e+03   |\n",
      "| n_updates               | 225245      |\n",
      "| policy_loss             | 587.35925   |\n",
      "| qf1_loss                | 1443.4645   |\n",
      "| qf2_loss                | 1404.2975   |\n",
      "| time_elapsed            | 4436        |\n",
      "| total timesteps         | 225500      |\n",
      "| value_loss              | 198.28119   |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| current_lr              | 0.0001        |\n",
      "| ent_coef                | 0.59355086    |\n",
      "| ent_coef_loss           | -0.0074644107 |\n",
      "| entropy                 | 0.24100597    |\n",
      "| episodes                | 456           |\n",
      "| fps                     | 50            |\n",
      "| mean 100 episode reward | -1.9e+03      |\n",
      "| n_updates               | 227245        |\n",
      "| policy_loss             | 768.29        |\n",
      "| qf1_loss                | 413.4541      |\n",
      "| qf2_loss                | 367.36783     |\n",
      "| time_elapsed            | 4479          |\n",
      "| total timesteps         | 227500        |\n",
      "| value_loss              | 296.408       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.5920491    |\n",
      "| ent_coef_loss           | -0.017842393 |\n",
      "| entropy                 | -0.19115707  |\n",
      "| episodes                | 460          |\n",
      "| fps                     | 50           |\n",
      "| mean 100 episode reward | -2.11e+03    |\n",
      "| n_updates               | 229245       |\n",
      "| policy_loss             | 656.94214    |\n",
      "| qf1_loss                | 620.58124    |\n",
      "| qf2_loss                | 530.8924     |\n",
      "| time_elapsed            | 4522         |\n",
      "| total timesteps         | 229500       |\n",
      "| value_loss              | 187.80093    |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.6014017  |\n",
      "| ent_coef_loss           | 0.26340145 |\n",
      "| entropy                 | 0.18072261 |\n",
      "| episodes                | 464        |\n",
      "| fps                     | 50         |\n",
      "| mean 100 episode reward | -1.79e+03  |\n",
      "| n_updates               | 231245     |\n",
      "| policy_loss             | 722.5156   |\n",
      "| qf1_loss                | 522.936    |\n",
      "| qf2_loss                | 512.3095   |\n",
      "| time_elapsed            | 4563       |\n",
      "| total timesteps         | 231500     |\n",
      "| value_loss              | 218.74252  |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.59414387  |\n",
      "| ent_coef_loss           | 0.13256158  |\n",
      "| entropy                 | 0.005288534 |\n",
      "| episodes                | 468         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -1.56e+03   |\n",
      "| n_updates               | 233245      |\n",
      "| policy_loss             | 702.4542    |\n",
      "| qf1_loss                | 3386.2307   |\n",
      "| qf2_loss                | 3418.9805   |\n",
      "| time_elapsed            | 4605        |\n",
      "| total timesteps         | 233500      |\n",
      "| value_loss              | 274.98807   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.59638745  |\n",
      "| ent_coef_loss           | -0.24367177 |\n",
      "| entropy                 | 0.21575063  |\n",
      "| episodes                | 472         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -1.92e+03   |\n",
      "| n_updates               | 235245      |\n",
      "| policy_loss             | 683.89453   |\n",
      "| qf1_loss                | 3368.0244   |\n",
      "| qf2_loss                | 3495.7915   |\n",
      "| time_elapsed            | 4646        |\n",
      "| total timesteps         | 235500      |\n",
      "| value_loss              | 196.46635   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.5885993   |\n",
      "| ent_coef_loss           | -0.15218775 |\n",
      "| entropy                 | 0.21007138  |\n",
      "| episodes                | 476         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.13e+03   |\n",
      "| n_updates               | 237245      |\n",
      "| policy_loss             | 602.5963    |\n",
      "| qf1_loss                | 966.64124   |\n",
      "| qf2_loss                | 981.11084   |\n",
      "| time_elapsed            | 4690        |\n",
      "| total timesteps         | 237500      |\n",
      "| value_loss              | 299.2561    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.60799414   |\n",
      "| ent_coef_loss           | -0.070332274 |\n",
      "| entropy                 | -0.12422888  |\n",
      "| episodes                | 480          |\n",
      "| fps                     | 50           |\n",
      "| mean 100 episode reward | -2.06e+03    |\n",
      "| n_updates               | 239245       |\n",
      "| policy_loss             | 747.68396    |\n",
      "| qf1_loss                | 8337.684     |\n",
      "| qf2_loss                | 8358.684     |\n",
      "| time_elapsed            | 4733         |\n",
      "| total timesteps         | 239500       |\n",
      "| value_loss              | 173.31729    |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.6099946   |\n",
      "| ent_coef_loss           | -0.08062619 |\n",
      "| entropy                 | -0.07218951 |\n",
      "| episodes                | 484         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -1.82e+03   |\n",
      "| n_updates               | 241245      |\n",
      "| policy_loss             | 847.546     |\n",
      "| qf1_loss                | 1384.5366   |\n",
      "| qf2_loss                | 1443.3291   |\n",
      "| time_elapsed            | 4774        |\n",
      "| total timesteps         | 241500      |\n",
      "| value_loss              | 323.78574   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.60112715  |\n",
      "| ent_coef_loss           | -0.14103283 |\n",
      "| entropy                 | 0.013885677 |\n",
      "| episodes                | 488         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.11e+03   |\n",
      "| n_updates               | 243245      |\n",
      "| policy_loss             | 876.2367    |\n",
      "| qf1_loss                | 339.02542   |\n",
      "| qf2_loss                | 364.50125   |\n",
      "| time_elapsed            | 4818        |\n",
      "| total timesteps         | 243500      |\n",
      "| value_loss              | 215.3723    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.60719126  |\n",
      "| ent_coef_loss           | -0.24408327 |\n",
      "| entropy                 | 0.1313667   |\n",
      "| episodes                | 492         |\n",
      "| fps                     | 50          |\n",
      "| mean 100 episode reward | -2.08e+03   |\n",
      "| n_updates               | 245245      |\n",
      "| policy_loss             | 713.41254   |\n",
      "| qf1_loss                | 607.5564    |\n",
      "| qf2_loss                | 588.1872    |\n",
      "| time_elapsed            | 4861        |\n",
      "| total timesteps         | 245500      |\n",
      "| value_loss              | 170.59619   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.61340624 |\n",
      "| ent_coef_loss           | 0.2204766  |\n",
      "| entropy                 | 0.20130119 |\n",
      "| episodes                | 496        |\n",
      "| fps                     | 50         |\n",
      "| mean 100 episode reward | -2e+03     |\n",
      "| n_updates               | 247245     |\n",
      "| policy_loss             | 696.05383  |\n",
      "| qf1_loss                | 521.75684  |\n",
      "| qf2_loss                | 504.3463   |\n",
      "| time_elapsed            | 4902       |\n",
      "| total timesteps         | 247500     |\n",
      "| value_loss              | 207.6123   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.6145701  |\n",
      "| ent_coef_loss           | 0.12000342 |\n",
      "| entropy                 | 0.24019983 |\n",
      "| episodes                | 500        |\n",
      "| fps                     | 50         |\n",
      "| mean 100 episode reward | -1.55e+03  |\n",
      "| n_updates               | 249245     |\n",
      "| policy_loss             | 689.85925  |\n",
      "| qf1_loss                | 1690.1404  |\n",
      "| qf2_loss                | 1815.7261  |\n",
      "| time_elapsed            | 4941       |\n",
      "| total timesteps         | 249500     |\n",
      "| value_loss              | 354.44794  |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exp_root = './experiments'\n",
    "hms_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# exp_name = 'HER_3F_drop_sparse_1e6'\n",
    "exp_name = 'SAC_Dclaw_1e6'\n",
    "\n",
    "exp_dir = osp.join(exp_root, exp_name, hms_time)\n",
    "os.makedirs(exp_dir)\n",
    "\n",
    "model = SAC('MlpPolicy', env,\n",
    "            tensorboard_log=exp_dir,\n",
    "            verbose=1, buffer_size=int(1e6),\n",
    "            learning_rate=1e-4,\n",
    "            batch_size=256,\n",
    "            policy_kwargs=dict(layers=[256, 256, 256]))\n",
    "\n",
    "# Train for 1e6 steps\n",
    "model.learn(int(2.5e5),)\n",
    "# Save the trained agent\n",
    "model.save(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.001       |\n",
      "| ent_coef                | 0.35916397  |\n",
      "| ent_coef_loss           | -0.57778233 |\n",
      "| entropy                 | 6.500439    |\n",
      "| episodes                | 100         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -6.45e+03   |\n",
      "| n_updates               | 49001       |\n",
      "| policy_loss             | -178.92224  |\n",
      "| qf1_loss                | 44.42558    |\n",
      "| qf2_loss                | 54.024677   |\n",
      "| time_elapsed            | 967         |\n",
      "| total timesteps         | 49500       |\n",
      "| value_loss              | 65.52342    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.001       |\n",
      "| ent_coef                | 0.32180473  |\n",
      "| ent_coef_loss           | -0.32120538 |\n",
      "| entropy                 | 5.6652527   |\n",
      "| episodes                | 200         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | -334        |\n",
      "| n_updates               | 99001       |\n",
      "| policy_loss             | -166.2712   |\n",
      "| qf1_loss                | 82.09961    |\n",
      "| qf2_loss                | 77.14152    |\n",
      "| time_elapsed            | 1950        |\n",
      "| total timesteps         | 99500       |\n",
      "| value_loss              | 97.28278    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.001       |\n",
      "| ent_coef                | 0.26946312  |\n",
      "| ent_coef_loss           | -0.86985874 |\n",
      "| entropy                 | 5.572976    |\n",
      "| episodes                | 300         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | 4.95e+03    |\n",
      "| n_updates               | 149001      |\n",
      "| policy_loss             | -193.66792  |\n",
      "| qf1_loss                | 85.00856    |\n",
      "| qf2_loss                | 83.529686   |\n",
      "| time_elapsed            | 2919        |\n",
      "| total timesteps         | 149500      |\n",
      "| value_loss              | 67.399124   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.001       |\n",
      "| ent_coef                | 0.23431881  |\n",
      "| ent_coef_loss           | -0.24434558 |\n",
      "| entropy                 | 5.62191     |\n",
      "| episodes                | 400         |\n",
      "| fps                     | 51          |\n",
      "| mean 100 episode reward | 6.5e+03     |\n",
      "| n_updates               | 199001      |\n",
      "| policy_loss             | -236.45122  |\n",
      "| qf1_loss                | 54.50591    |\n",
      "| qf2_loss                | 50.868095   |\n",
      "| time_elapsed            | 3857        |\n",
      "| total timesteps         | 199500      |\n",
      "| value_loss              | 85.562126   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.001      |\n",
      "| ent_coef                | 0.1687594  |\n",
      "| ent_coef_loss           | -1.7241678 |\n",
      "| entropy                 | 6.180108   |\n",
      "| episodes                | 500        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | 6.7e+03    |\n",
      "| n_updates               | 249001     |\n",
      "| policy_loss             | -282.81564 |\n",
      "| qf1_loss                | 24.387905  |\n",
      "| qf2_loss                | 38.47677   |\n",
      "| time_elapsed            | 4813       |\n",
      "| total timesteps         | 249500     |\n",
      "| value_loss              | 53.342243  |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.001      |\n",
      "| ent_coef                | 0.14854296 |\n",
      "| ent_coef_loss           | 0.4503027  |\n",
      "| entropy                 | 6.2682962  |\n",
      "| episodes                | 600        |\n",
      "| fps                     | 51         |\n",
      "| mean 100 episode reward | 7.68e+03   |\n",
      "| n_updates               | 299001     |\n",
      "| policy_loss             | -299.75632 |\n",
      "| qf1_loss                | 21.080568  |\n",
      "| qf2_loss                | 23.210691  |\n",
      "| time_elapsed            | 5778       |\n",
      "| total timesteps         | 299500     |\n",
      "| value_loss              | 34.33441   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-72f60770a235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Train for 1e6 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# Save the trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/her/her.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    111\u001b[0m         return self.model.learn(total_timesteps, callback=callback, log_interval=log_interval,\n\u001b[1;32m    112\u001b[0m                                 \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                                 replay_wrapper=self.replay_wrapper)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/sac/sac.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m    462\u001b[0m                         \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                         \u001b[0;31m# Update policy and critics (q functions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                         \u001b[0mmb_infos_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m                         \u001b[0;31m# Update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgrad_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_update_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/stable_baselines/sac/sac.py\u001b[0m in \u001b[0;36m_train_step\u001b[0;34m(self, step, writer, learning_rate)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;31m# and optionally compute log for tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scr1/.pyenv/versions/3.6.6/envs/safegym/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "exp_root = './experiments'\n",
    "hms_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# exp_name = 'HER_3F_drop_sparse_1e6'\n",
    "exp_name = 'HER_Dclaw_1e6'\n",
    "\n",
    "exp_dir = osp.join(exp_root, exp_name, hms_time)\n",
    "os.makedirs(exp_dir)\n",
    "\n",
    "model = HER('MlpPolicy', env, SAC, n_sampled_goal=4,\n",
    "            tensorboard_log=exp_dir,\n",
    "            goal_selection_strategy='future',\n",
    "            verbose=1, buffer_size=int(1e6),\n",
    "            learning_rate=1e-3,\n",
    "            gamma=0.95, batch_size=256,\n",
    "            policy_kwargs=dict(layers=[256, 256, 256]))\n",
    "\n",
    "# Train for 1e6 steps\n",
    "model.learn(int(1e6),)\n",
    "# Save the trained agent\n",
    "model.save(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(exp_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

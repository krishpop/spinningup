{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import os.path as osp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines import HER, SAC\n",
    "from stable_baselines.common.atari_wrappers import FrameStack\n",
    "from rrc_simulation.gym_wrapper.envs import custom_env, cube_env\n",
    "from rrc_simulation.tasks import move_cube\n",
    "from spinup.utils import rrc_utils\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reorient_env():\n",
    "    info_keys = ['is_success', 'is_success_ori_dist', 'dist', 'final_dist', 'final_score',\n",
    "                 'final_ori_dist']\n",
    "\n",
    "    wrappers = [gym.wrappers.ClipAction,\n",
    "                functools.partial(custom_env.LogInfoWrapper, info_keys=info_keys),\n",
    "                functools.partial(custom_env.CubeRewardWrapper, pos_coef=1., ori_coef=1., fingertip_coef=1.),\n",
    "                                ac_norm_pen=0.2, rew_fn='exp',\n",
    "                                goal_env=True)},\n",
    "                {'cls': custom_env.ReorientWrapper,\n",
    "                 'kwargs': dict(goal_env=True)},\n",
    "                {'cls': gym.wrappers.TimeLimit,\n",
    "                 'kwargs': dict(max_episode_steps=rrc_utils.EPLEN)},\n",
    "                functools.partial(custom_env.ScaledActionWrapper,\n",
    "                    goal_env=False, relative=True),\n",
    "                functools.partial(wrappers.TimeLimit, max_episode_steps=EPLEN_SHORT*2),\n",
    "                    reorient_log_info_wrapper,\n",
    "                wrappers.FlattenObservation]\n",
    "\n",
    "    initializer = custom_env.ReorientInitializer(1, 0.1)\n",
    "    env_fn = rrc_utils.make_env_fn('real_robot_challenge_phase_1-v1', wrapper_params=wrappers,\n",
    "                                   action_type=rrc_utils.action_type,\n",
    "                                   initializer=initializer,\n",
    "                                   frameskip=rrc_utils.FRAMESKIP,\n",
    "                                   visualization=False)\n",
    "    env = env_fn()\n",
    "    return env\n",
    "\n",
    "\n",
    "def make_curr_env():\n",
    "    info_keys = ['is_success_ori_dist', 'dist', 'final_dist', 'final_score',\n",
    "                 'final_ori_dist', 'goal_sample_radius',\n",
    "                 'init_sample_radius']\n",
    "\n",
    "    wrappers = [gym.wrappers.ClipAction, \n",
    "                {'cls': custom_env.LogInfoWrapper,\n",
    "                 'kwargs': dict(info_keys=info_keys)},\n",
    "                {'cls': gym.wrappers.TimeLimit,\n",
    "                 'kwargs': dict(max_episode_steps=rrc_utils.EPLEN)},\n",
    "                custom_env.FlattenGoalWrapper,]\n",
    "\n",
    "    env_fn = rrc_utils.make_env_fn('real_robot_challenge_phase_1-v4', wrapper_params=wrappers,\n",
    "                                   action_type=rrc_utils.action_type,\n",
    "                                   initializer=rrc_utils.push_curr_initializer,\n",
    "                                   frameskip=rrc_utils.FRAMESKIP,\n",
    "                                   visualization=False)\n",
    "    env = env_fn()\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_reorient_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a model without an environment, this model cannot be trained until it has a valid environment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.her.her.HER at 0x7fd9d5b30be0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dir = './data/HER-SAC_sparse_push/2020-09-18_12-28-22/2e6-steps.zip' # './data/HER-SAC_push_reorient/2020-09-20_15-58-02/1e6-steps.zip'\n",
    "model.load(load_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8761622 ,  1.4965066 , -0.0684433 , -0.01645833,  0.9974102 ,\n",
       "       -0.5804248 , -0.24998152,  1.4394349 , -2.669657  ], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "937.5\n"
     ]
    }
   ],
   "source": [
    "n_eps = 10\n",
    "final_infos = []\n",
    "\n",
    "initial_pose = move_cube.Pose(np.array([0.02176933,0.11905757,0.0325]),\n",
    "                              np.array([0,0,0.47478757,0.88010043]))\n",
    "goal_pose =  move_cube.Pose(position=np.array([0,0,0.0825]), orientation=np.array([0,0,0,1]))\n",
    "\n",
    "initializer = cube_env.FixedInitializer(\n",
    "    2, initial_pose, goal_pose\n",
    ")\n",
    "env.unwrapped.initializer = initializer\n",
    "\n",
    "for _ in range(n_eps):\n",
    "    d = False\n",
    "    obs = env.reset()\n",
    "\n",
    "    r_total = 0\n",
    "    while not d and not i.get('is_success'):\n",
    "        obs, r, d, i = env.step(model.predict(obs)[0])\n",
    "        r_total += r\n",
    "    i['total_rew'] = r_total\n",
    "    final_infos.append(i)\n",
    "    \n",
    "print(np.mean([i['total_rew'] for i in final_infos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12102928082417459, 0.12102928082417459, 0.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min([i['final_dist'] for i in final_infos]), np.mean([i['final_dist'] for i in final_infos]), np.std([i['final_dist'] for i in final_infos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.027137475 |\n",
      "| ent_coef_loss           | -23.953201  |\n",
      "| entropy                 | 8.92744     |\n",
      "| episodes                | 100         |\n",
      "| fps                     | 40          |\n",
      "| mean 100 episode reward | 343         |\n",
      "| n_updates               | 36751       |\n",
      "| policy_loss             | -43.370445  |\n",
      "| qf1_loss                | 2.311096    |\n",
      "| qf2_loss                | 2.086415    |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 918         |\n",
      "| total timesteps         | 37125       |\n",
      "| value_loss              | 0.2303352   |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.007934947 |\n",
      "| ent_coef_loss           | 1.1313438   |\n",
      "| entropy                 | 7.422938    |\n",
      "| episodes                | 200         |\n",
      "| fps                     | 36          |\n",
      "| mean 100 episode reward | 412         |\n",
      "| n_updates               | 74251       |\n",
      "| policy_loss             | -43.763683  |\n",
      "| qf1_loss                | 0.19630106  |\n",
      "| qf2_loss                | 0.2695016   |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 2026        |\n",
      "| total timesteps         | 74625       |\n",
      "| value_loss              | 0.14490423  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.006973123 |\n",
      "| ent_coef_loss           | 1.8559548   |\n",
      "| entropy                 | 8.246805    |\n",
      "| episodes                | 300         |\n",
      "| fps                     | 35          |\n",
      "| mean 100 episode reward | 400         |\n",
      "| n_updates               | 111751      |\n",
      "| policy_loss             | -43.120274  |\n",
      "| qf1_loss                | 0.16360003  |\n",
      "| qf2_loss                | 0.1654812   |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 3170        |\n",
      "| total timesteps         | 112125      |\n",
      "| value_loss              | 0.09051184  |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0060455124 |\n",
      "| ent_coef_loss           | -4.131319    |\n",
      "| entropy                 | 8.211897     |\n",
      "| episodes                | 400          |\n",
      "| fps                     | 33           |\n",
      "| mean 100 episode reward | 432          |\n",
      "| n_updates               | 149251       |\n",
      "| policy_loss             | -45.422813   |\n",
      "| qf1_loss                | 0.2507579    |\n",
      "| qf2_loss                | 0.25892496   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 4417         |\n",
      "| total timesteps         | 149625       |\n",
      "| value_loss              | 0.07557507   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.005520191 |\n",
      "| ent_coef_loss           | 4.0760064   |\n",
      "| entropy                 | 7.9616365   |\n",
      "| episodes                | 500         |\n",
      "| fps                     | 32          |\n",
      "| mean 100 episode reward | 442         |\n",
      "| n_updates               | 186751      |\n",
      "| policy_loss             | -42.51085   |\n",
      "| qf1_loss                | 0.3836043   |\n",
      "| qf2_loss                | 0.4206831   |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 5701        |\n",
      "| total timesteps         | 187125      |\n",
      "| value_loss              | 0.08011977  |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0052219904 |\n",
      "| ent_coef_loss           | 0.86735225   |\n",
      "| entropy                 | 8.023636     |\n",
      "| episodes                | 600          |\n",
      "| fps                     | 31           |\n",
      "| mean 100 episode reward | 387          |\n",
      "| n_updates               | 224251       |\n",
      "| policy_loss             | -43.381763   |\n",
      "| qf1_loss                | 0.3051077    |\n",
      "| qf2_loss                | 0.26539278   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 7036         |\n",
      "| total timesteps         | 224625       |\n",
      "| value_loss              | 0.15995805   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.00512475  |\n",
      "| ent_coef_loss           | -2.2566905  |\n",
      "| entropy                 | 7.755234    |\n",
      "| episodes                | 700         |\n",
      "| fps                     | 31          |\n",
      "| mean 100 episode reward | 474         |\n",
      "| n_updates               | 261751      |\n",
      "| policy_loss             | -42.77131   |\n",
      "| qf1_loss                | 0.4472615   |\n",
      "| qf2_loss                | 0.36583403  |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 8400        |\n",
      "| total timesteps         | 262125      |\n",
      "| value_loss              | 0.059302375 |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0053714397 |\n",
      "| ent_coef_loss           | -1.8914413   |\n",
      "| entropy                 | 8.038619     |\n",
      "| episodes                | 800          |\n",
      "| fps                     | 30           |\n",
      "| mean 100 episode reward | 372          |\n",
      "| n_updates               | 299251       |\n",
      "| policy_loss             | -44.825966   |\n",
      "| qf1_loss                | 0.19098371   |\n",
      "| qf2_loss                | 0.16776681   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 9701         |\n",
      "| total timesteps         | 299625       |\n",
      "| value_loss              | 0.06483257   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.005812194 |\n",
      "| ent_coef_loss           | 2.0845919   |\n",
      "| entropy                 | 7.6772437   |\n",
      "| episodes                | 900         |\n",
      "| fps                     | 30          |\n",
      "| mean 100 episode reward | 441         |\n",
      "| n_updates               | 336751      |\n",
      "| policy_loss             | -44.78625   |\n",
      "| qf1_loss                | 0.06309594  |\n",
      "| qf2_loss                | 0.072420284 |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 10988       |\n",
      "| total timesteps         | 337125      |\n",
      "| value_loss              | 0.05528162  |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0058043865 |\n",
      "| ent_coef_loss           | 3.092106     |\n",
      "| entropy                 | 7.5280027    |\n",
      "| episodes                | 1000         |\n",
      "| fps                     | 30           |\n",
      "| mean 100 episode reward | 441          |\n",
      "| n_updates               | 374251       |\n",
      "| policy_loss             | -43.884354   |\n",
      "| qf1_loss                | 0.063840136  |\n",
      "| qf2_loss                | 0.06841235   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 12386        |\n",
      "| total timesteps         | 374625       |\n",
      "| value_loss              | 0.07950832   |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0058162585 |\n",
      "| ent_coef_loss           | 2.4268842    |\n",
      "| entropy                 | 7.835534     |\n",
      "| episodes                | 1100         |\n",
      "| fps                     | 29           |\n",
      "| mean 100 episode reward | 427          |\n",
      "| n_updates               | 411751       |\n",
      "| policy_loss             | -43.030396   |\n",
      "| qf1_loss                | 0.22548571   |\n",
      "| qf2_loss                | 0.18624799   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 13737        |\n",
      "| total timesteps         | 412125       |\n",
      "| value_loss              | 0.12221806   |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0057185357 |\n",
      "| ent_coef_loss           | -5.576524    |\n",
      "| entropy                 | 7.511784     |\n",
      "| episodes                | 1200         |\n",
      "| fps                     | 29           |\n",
      "| mean 100 episode reward | 424          |\n",
      "| n_updates               | 449251       |\n",
      "| policy_loss             | -43.171223   |\n",
      "| qf1_loss                | 0.085776374  |\n",
      "| qf2_loss                | 0.086473     |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 15062        |\n",
      "| total timesteps         | 449625       |\n",
      "| value_loss              | 0.038299575  |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0059965416 |\n",
      "| ent_coef_loss           | 0.8848677    |\n",
      "| entropy                 | 7.521302     |\n",
      "| episodes                | 1400         |\n",
      "| fps                     | 29           |\n",
      "| mean 100 episode reward | 484          |\n",
      "| n_updates               | 524251       |\n",
      "| policy_loss             | -42.589684   |\n",
      "| qf1_loss                | 0.22478645   |\n",
      "| qf2_loss                | 0.22480299   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 17821        |\n",
      "| total timesteps         | 524625       |\n",
      "| value_loss              | 0.07058355   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.00558306  |\n",
      "| ent_coef_loss           | -5.2169294  |\n",
      "| entropy                 | 7.305898    |\n",
      "| episodes                | 1500        |\n",
      "| fps                     | 29          |\n",
      "| mean 100 episode reward | 472         |\n",
      "| n_updates               | 561751      |\n",
      "| policy_loss             | -44.037315  |\n",
      "| qf1_loss                | 0.0937182   |\n",
      "| qf2_loss                | 0.086805664 |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 19137       |\n",
      "| total timesteps         | 562125      |\n",
      "| value_loss              | 0.07624327  |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0059250067 |\n",
      "| ent_coef_loss           | 3.1617613    |\n",
      "| entropy                 | 7.303217     |\n",
      "| episodes                | 1600         |\n",
      "| fps                     | 29           |\n",
      "| mean 100 episode reward | 480          |\n",
      "| n_updates               | 599251       |\n",
      "| policy_loss             | -44.453423   |\n",
      "| qf1_loss                | 0.03856678   |\n",
      "| qf2_loss                | 0.046302922  |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 20480        |\n",
      "| total timesteps         | 599625       |\n",
      "| value_loss              | 0.05472991   |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0063286438 |\n",
      "| ent_coef_loss           | -2.0738118   |\n",
      "| entropy                 | 7.7265177    |\n",
      "| episodes                | 1700         |\n",
      "| fps                     | 29           |\n",
      "| mean 100 episode reward | 491          |\n",
      "| n_updates               | 636751       |\n",
      "| policy_loss             | -45.82218    |\n",
      "| qf1_loss                | 0.06733663   |\n",
      "| qf2_loss                | 0.05252024   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 21904        |\n",
      "| total timesteps         | 637125       |\n",
      "| value_loss              | 0.06978433   |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0064041354 |\n",
      "| ent_coef_loss           | -1.807745    |\n",
      "| entropy                 | 7.668316     |\n",
      "| episodes                | 1800         |\n",
      "| fps                     | 28           |\n",
      "| mean 100 episode reward | 414          |\n",
      "| n_updates               | 674251       |\n",
      "| policy_loss             | -45.634693   |\n",
      "| qf1_loss                | 0.0585597    |\n",
      "| qf2_loss                | 0.05205751   |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 23384        |\n",
      "| total timesteps         | 674625       |\n",
      "| value_loss              | 0.04492932   |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0055149687 |\n",
      "| ent_coef_loss           | 2.4177608    |\n",
      "| entropy                 | 7.3179297    |\n",
      "| episodes                | 1900         |\n",
      "| fps                     | 28           |\n",
      "| mean 100 episode reward | 505          |\n",
      "| n_updates               | 711751       |\n",
      "| policy_loss             | -44.5667     |\n",
      "| qf1_loss                | 3.878676     |\n",
      "| qf2_loss                | 3.8360982    |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 24743        |\n",
      "| total timesteps         | 712125       |\n",
      "| value_loss              | 0.034424253  |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0065998766 |\n",
      "| ent_coef_loss           | -6.2323065   |\n",
      "| entropy                 | 7.5431542    |\n",
      "| episodes                | 2000         |\n",
      "| fps                     | 28           |\n",
      "| mean 100 episode reward | 430          |\n",
      "| n_updates               | 749251       |\n",
      "| policy_loss             | -45.356      |\n",
      "| qf1_loss                | 0.046205107  |\n",
      "| qf2_loss                | 0.050289202  |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 26134        |\n",
      "| total timesteps         | 749625       |\n",
      "| value_loss              | 0.05621478   |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.006351284 |\n",
      "| ent_coef_loss           | -2.5608213  |\n",
      "| entropy                 | 7.4041286   |\n",
      "| episodes                | 2100        |\n",
      "| fps                     | 28          |\n",
      "| mean 100 episode reward | 492         |\n",
      "| n_updates               | 786751      |\n",
      "| policy_loss             | -43.365112  |\n",
      "| qf1_loss                | 0.042165697 |\n",
      "| qf2_loss                | 0.033856165 |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 27474       |\n",
      "| total timesteps         | 787125      |\n",
      "| value_loss              | 0.070825234 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.006125797 |\n",
      "| ent_coef_loss           | 2.2717724   |\n",
      "| entropy                 | 7.165028    |\n",
      "| episodes                | 2200        |\n",
      "| fps                     | 28          |\n",
      "| mean 100 episode reward | 529         |\n",
      "| n_updates               | 824251      |\n",
      "| policy_loss             | -44.553337  |\n",
      "| qf1_loss                | 0.03859055  |\n",
      "| qf2_loss                | 0.033263642 |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 28842       |\n",
      "| total timesteps         | 824625      |\n",
      "| value_loss              | 0.04310093  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.005304306 |\n",
      "| ent_coef_loss           | 3.0444853   |\n",
      "| entropy                 | 6.8140755   |\n",
      "| episodes                | 2300        |\n",
      "| fps                     | 28          |\n",
      "| mean 100 episode reward | 464         |\n",
      "| n_updates               | 861751      |\n",
      "| policy_loss             | -43.129967  |\n",
      "| qf1_loss                | 0.07747602  |\n",
      "| qf2_loss                | 0.06920319  |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 30292       |\n",
      "| total timesteps         | 862125      |\n",
      "| value_loss              | 0.067088895 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.005702761 |\n",
      "| ent_coef_loss           | -2.5637927  |\n",
      "| entropy                 | 7.0266857   |\n",
      "| episodes                | 2400        |\n",
      "| fps                     | 28          |\n",
      "| mean 100 episode reward | 508         |\n",
      "| n_updates               | 899251      |\n",
      "| policy_loss             | -42.920547  |\n",
      "| qf1_loss                | 0.08016492  |\n",
      "| qf2_loss                | 0.06466967  |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 31670       |\n",
      "| total timesteps         | 899625      |\n",
      "| value_loss              | 0.041592132 |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| current_lr              | 0.0001       |\n",
      "| ent_coef                | 0.0061534136 |\n",
      "| ent_coef_loss           | 2.137011     |\n",
      "| entropy                 | 7.181079     |\n",
      "| episodes                | 2500         |\n",
      "| fps                     | 28           |\n",
      "| mean 100 episode reward | 489          |\n",
      "| n_updates               | 936751       |\n",
      "| policy_loss             | -45.451164   |\n",
      "| qf1_loss                | 0.15570861   |\n",
      "| qf2_loss                | 0.117850125  |\n",
      "| success rate            | 0            |\n",
      "| time_elapsed            | 33061        |\n",
      "| total timesteps         | 937125       |\n",
      "| value_loss              | 0.034290034  |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.005829525 |\n",
      "| ent_coef_loss           | -2.707197   |\n",
      "| entropy                 | 7.341038    |\n",
      "| episodes                | 2600        |\n",
      "| fps                     | 28          |\n",
      "| mean 100 episode reward | 501         |\n",
      "| n_updates               | 974251      |\n",
      "| policy_loss             | -43.73419   |\n",
      "| qf1_loss                | 0.03665875  |\n",
      "| qf2_loss                | 0.060379013 |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 34467       |\n",
      "| total timesteps         | 974625      |\n",
      "| value_loss              | 0.092788406 |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "exp_root = './data'\n",
    "hms_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "exp_name = 'HER-SAC_push_reorient'\n",
    "exp_dir = osp.join(exp_root, exp_name, hms_time)\n",
    "os.makedirs(exp_dir)\n",
    "\n",
    "model = HER('MlpPolicy', env, SAC, n_sampled_goal=4,\n",
    "            tensorboard_log=exp_dir,\n",
    "            goal_selection_strategy='future',\n",
    "            verbose=1, buffer_size=int(1e6),\n",
    "            learning_rate=1e-4,\n",
    "            gamma=0.95, batch_size=256,\n",
    "            policy_kwargs=dict(layers=[256, 256]))\n",
    "\n",
    "# Train for 1e6 steps\n",
    "model.learn(int(1e6),)\n",
    "# Save the trained agent\n",
    "model.save(osp.join(exp_dir, '1e6-steps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.023150394 |\n",
      "| ent_coef_loss           | -2.192941   |\n",
      "| entropy                 | 7.891837    |\n",
      "| episodes                | 100         |\n",
      "| fps                     | 27          |\n",
      "| mean 100 episode reward | 152         |\n",
      "| n_updates               | 37125       |\n",
      "| policy_loss             | -21.213112  |\n",
      "| qf1_loss                | 0.28706837  |\n",
      "| qf2_loss                | 0.23305702  |\n",
      "| success rate            | 0.0404      |\n",
      "| time_elapsed            | 1331        |\n",
      "| total timesteps         | 1037125     |\n",
      "| value_loss              | 0.23443252  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.014548837 |\n",
      "| ent_coef_loss           | 0.875576    |\n",
      "| entropy                 | 7.7432957   |\n",
      "| episodes                | 400         |\n",
      "| fps                     | 26          |\n",
      "| mean 100 episode reward | 3.9         |\n",
      "| n_updates               | 149625      |\n",
      "| policy_loss             | -19.195568  |\n",
      "| qf1_loss                | 0.24412605  |\n",
      "| qf2_loss                | 0.19066933  |\n",
      "| success rate            | 0           |\n",
      "| time_elapsed            | 5568        |\n",
      "| total timesteps         | 1149625     |\n",
      "| value_loss              | 0.10292999  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.01214371  |\n",
      "| ent_coef_loss           | 2.1730347   |\n",
      "| entropy                 | 7.5480127   |\n",
      "| episodes                | 500         |\n",
      "| fps                     | 26          |\n",
      "| mean 100 episode reward | 39          |\n",
      "| n_updates               | 187125      |\n",
      "| policy_loss             | -18.6596    |\n",
      "| qf1_loss                | 0.068811215 |\n",
      "| qf2_loss                | 0.061896127 |\n",
      "| success rate            | 0.01        |\n",
      "| time_elapsed            | 7128        |\n",
      "| total timesteps         | 1187125     |\n",
      "| value_loss              | 0.061093498 |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.01569116 |\n",
      "| ent_coef_loss           | -1.521354  |\n",
      "| entropy                 | 7.6589947  |\n",
      "| episodes                | 700        |\n",
      "| fps                     | 25         |\n",
      "| mean 100 episode reward | 117        |\n",
      "| n_updates               | 262125     |\n",
      "| policy_loss             | -20.83873  |\n",
      "| qf1_loss                | 0.22742945 |\n",
      "| qf2_loss                | 0.2335444  |\n",
      "| success rate            | 0.03       |\n",
      "| time_elapsed            | 10210      |\n",
      "| total timesteps         | 1262125    |\n",
      "| value_loss              | 0.27796218 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.017116867 |\n",
      "| ent_coef_loss           | -2.787228   |\n",
      "| entropy                 | 7.5751915   |\n",
      "| episodes                | 800         |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 74.4        |\n",
      "| n_updates               | 299625      |\n",
      "| policy_loss             | -18.970015  |\n",
      "| qf1_loss                | 0.0719721   |\n",
      "| qf2_loss                | 0.038611002 |\n",
      "| success rate            | 0.02        |\n",
      "| time_elapsed            | 11732       |\n",
      "| total timesteps         | 1299625     |\n",
      "| value_loss              | 0.06559506  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.014339598 |\n",
      "| ent_coef_loss           | 5.341829    |\n",
      "| entropy                 | 7.957736    |\n",
      "| episodes                | 1200        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 116         |\n",
      "| n_updates               | 449625      |\n",
      "| policy_loss             | -18.207825  |\n",
      "| qf1_loss                | 0.55145395  |\n",
      "| qf2_loss                | 0.30685058  |\n",
      "| success rate            | 0.03        |\n",
      "| time_elapsed            | 17650       |\n",
      "| total timesteps         | 1449625     |\n",
      "| value_loss              | 0.29517898  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.017647773 |\n",
      "| ent_coef_loss           | 2.055232    |\n",
      "| entropy                 | 7.265978    |\n",
      "| episodes                | 1400        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 300         |\n",
      "| n_updates               | 524625      |\n",
      "| policy_loss             | -19.787943  |\n",
      "| qf1_loss                | 1.6641762   |\n",
      "| qf2_loss                | 0.64769304  |\n",
      "| success rate            | 0.08        |\n",
      "| time_elapsed            | 20525       |\n",
      "| total timesteps         | 1524625     |\n",
      "| value_loss              | 0.09093179  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.018976307 |\n",
      "| ent_coef_loss           | 2.1867702   |\n",
      "| entropy                 | 7.754022    |\n",
      "| episodes                | 1500        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 265         |\n",
      "| n_updates               | 562125      |\n",
      "| policy_loss             | -21.47784   |\n",
      "| qf1_loss                | 56.658756   |\n",
      "| qf2_loss                | 57.584606   |\n",
      "| success rate            | 0.07        |\n",
      "| time_elapsed            | 22019       |\n",
      "| total timesteps         | 1562125     |\n",
      "| value_loss              | 0.119604066 |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.015864322 |\n",
      "| ent_coef_loss           | -0.17654438 |\n",
      "| entropy                 | 7.9250298   |\n",
      "| episodes                | 1600        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 159         |\n",
      "| n_updates               | 599625      |\n",
      "| policy_loss             | -20.58025   |\n",
      "| qf1_loss                | 0.30058303  |\n",
      "| qf2_loss                | 0.14109597  |\n",
      "| success rate            | 0.05        |\n",
      "| time_elapsed            | 23532       |\n",
      "| total timesteps         | 1599625     |\n",
      "| value_loss              | 0.22474808  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.018178575 |\n",
      "| ent_coef_loss           | 1.5138202   |\n",
      "| entropy                 | 7.627286    |\n",
      "| episodes                | 1700        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 334         |\n",
      "| n_updates               | 637125      |\n",
      "| policy_loss             | -21.832607  |\n",
      "| qf1_loss                | 0.1689137   |\n",
      "| qf2_loss                | 0.15433837  |\n",
      "| success rate            | 0.09        |\n",
      "| time_elapsed            | 25001       |\n",
      "| total timesteps         | 1637125     |\n",
      "| value_loss              | 0.16614795  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| current_lr              | 0.0001     |\n",
      "| ent_coef                | 0.01515324 |\n",
      "| ent_coef_loss           | 0.7676546  |\n",
      "| entropy                 | 7.538866   |\n",
      "| episodes                | 1800       |\n",
      "| fps                     | 25         |\n",
      "| mean 100 episode reward | 188        |\n",
      "| n_updates               | 674625     |\n",
      "| policy_loss             | -20.875153 |\n",
      "| qf1_loss                | 0.32570827 |\n",
      "| qf2_loss                | 0.3361346  |\n",
      "| success rate            | 0.05       |\n",
      "| time_elapsed            | 26451      |\n",
      "| total timesteps         | 1674625    |\n",
      "| value_loss              | 0.10750654 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.020806171 |\n",
      "| ent_coef_loss           | 5.100204    |\n",
      "| entropy                 | 7.4131026   |\n",
      "| episodes                | 1900        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 188         |\n",
      "| n_updates               | 712125      |\n",
      "| policy_loss             | -24.218237  |\n",
      "| qf1_loss                | 0.59274995  |\n",
      "| qf2_loss                | 0.17903566  |\n",
      "| success rate            | 0.05        |\n",
      "| time_elapsed            | 27965       |\n",
      "| total timesteps         | 1712125     |\n",
      "| value_loss              | 0.40184855  |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| current_lr              | 0.0001      |\n",
      "| ent_coef                | 0.014200136 |\n",
      "| ent_coef_loss           | 0.15597618  |\n",
      "| entropy                 | 7.846473    |\n",
      "| episodes                | 3200        |\n",
      "| fps                     | 25          |\n",
      "| mean 100 episode reward | 262         |\n",
      "| n_updates               | 1199625     |\n",
      "| policy_loss             | -21.319262  |\n",
      "| qf1_loss                | 0.094568714 |\n",
      "| qf2_loss                | 0.09313113  |\n",
      "| success rate            | 0.07        |\n",
      "| time_elapsed            | 47389       |\n",
      "| total timesteps         | 2199625     |\n",
      "| value_loss              | 0.16986726  |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train for 1e6 steps\n",
    "model.learn(int(2e6),reset_num_timesteps=False)\n",
    "# Save the trained agent\n",
    "model.save(osp.join(exp_dir, '2e6-steps'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e6-steps.zip  HER_1\n"
     ]
    }
   ],
   "source": [
    "!ls $exp_dir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
